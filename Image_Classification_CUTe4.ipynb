{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"E://Insofe//Python Lab//10_class_ic_input/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"E://Insofe//Python Lab//10_categories/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders for  training data and validation randomly splitted \n",
    "new_train = os.mkdir(\"new_train\")\n",
    "new_valid = os.mkdir(\"new_valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_dir = \"E:/Insofe/Python Lab/10_class_ic_input/new_train/\"\n",
    "new_valid_dir = \"E:/Insofe/Python Lab/10_class_ic_input/new_valid/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:/Insofe/Python Lab/10_class_ic_input/new_train/'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:/Insofe/Python Lab/10_class_ic_input/new_valid/'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_valid_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = os.listdir(train_dir) # Get all the classes from the original train directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplanes',\n",
       " 'BACKGROUND_Google',\n",
       " 'bonsai',\n",
       " 'car_side',\n",
       " 'Faces',\n",
       " 'Faces_easy',\n",
       " 'grand_piano',\n",
       " 'Leopards',\n",
       " 'Motorbikes',\n",
       " 'watch']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target folders in the new train data directory\n",
    "for c in classes:\n",
    "    os.mkdir(new_train_dir+c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation folders in the new validation data directory\n",
    "for c in classes:\n",
    "    os.mkdir(new_valid_dir+c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path= train_dir+classes[0]+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for species in classes:\n",
    "    data_root = train_dir+species+'/'\n",
    "    data_new_train = new_train_dir + species + '/'\n",
    "    data_new_valid = new_valid_dir + species + '/'\n",
    "    data_root_list = os.listdir(data_root)\n",
    "    test_size = 0.8\n",
    "    split_index = int(0.8*len(data_root_list))\n",
    "    np.random.shuffle(data_root_list)\n",
    "    train_list = data_root_list[:split_index]\n",
    "    valid_list = data_root_list[split_index:]\n",
    "    for image in train_list :\n",
    "        source = data_root + image\n",
    "        target = data_new_train + image\n",
    "        shutil.copy(source,target)\n",
    "    for image in valid_list:\n",
    "        source = data_root + image\n",
    "        target = data_new_valid + image\n",
    "        shutil.copy(source,target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no = []\n",
    "valid_no = []\n",
    "for c in classes:\n",
    "    train_no.append((len(os.listdir(new_train_dir+c+'/'))))\n",
    "    valid_no.append(len(os.listdir(new_valid_dir+c+'/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2977"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(valid_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training airplane images: 640\n",
      "total validation airplane images: 160\n"
     ]
    }
   ],
   "source": [
    "train_airplanes_dir = \"E://Insofe//Python Lab//10_class_ic_input//new_train//airplanes/\"\n",
    "validation_airplanes_dir = \"E://Insofe//Python Lab//10_class_ic_input//new_valid/airplanes/\"\n",
    "print('total training airplane images:', len(os.listdir(train_airplanes_dir)))\n",
    "print('total validation airplane images:', len(os.listdir(validation_airplanes_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train BACKGROUND_Google  images: 373\n",
      "total validation validation airplane images: 94\n"
     ]
    }
   ],
   "source": [
    "train_BACKGROUND_Google_dir = \"E://Insofe//Python Lab//10_class_ic_input//new_train/BACKGROUND_Google/\"\n",
    "validation_BACKGROUND_Google_dir = \"E://Insofe//Python Lab//10_class_ic_input//new_valid/BACKGROUND_Google/\"\n",
    "print('total train BACKGROUND_Google  images:', len(os.listdir(train_BACKGROUND_Google_dir)))\n",
    "print('total validation validation airplane images:', len(os.listdir(validation_BACKGROUND_Google_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(10,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 3,457,738\n",
      "Trainable params: 3,457,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "model.compile(optimizer='RMSprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know by now, data should be formatted into appropriately preprocessed floatingpoint tensors before being fed into the network. Currently, the data sits on a drive as\n",
    "JPEG files, so the steps for getting it into the network are roughly as follows:\n",
    "1 Read the picture files.\n",
    "2 Decode the JPEG content to RGB grids of pixels.\n",
    "3 Convert these into floating-point tensors.\n",
    "4 Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know,\n",
    "neural networks prefer to deal with small input values).\n",
    "It may seem a bit daunting, but fortunately Keras has utilities to take care of these\n",
    "steps automatically. Keras has a module with image-processing helper tools, located at\n",
    "keras.preprocessing.image. In particular, it contains the class ImageDataGenerator,\n",
    "which lets you quickly set up Python generators that can automatically turn image files\n",
    "on disk into batches of preprocessed tensors. This is what you’ll use here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ImageDataGenerator to read images from directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"E:\\\\Insofe\\\\Python Lab\\\\10_class_ic_input\\\\new_train\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dir = \"E:\\\\Insofe\\\\Python Lab\\\\10_class_ic_input\\\\new_valid\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2977 images belonging to 10 classes.\n",
      "Found 747 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=(150, 150),\n",
    "                                                    batch_size=20,\n",
    "                                                    class_mode='categorical')\n",
    "validation_generator = validation_datagen.flow_from_directory(validation_dir,\n",
    "                                                              target_size=(150, 150),\n",
    "                                                              batch_size=20,\n",
    "                                                              class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: (20, 10)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model using a batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "100/100 [==============================] - 138s 1s/step - loss: 1.6968 - acc: 0.4625 - val_loss: 0.8614 - val_acc: 0.7138\n",
      "Epoch 2/25\n",
      "100/100 [==============================] - 133s 1s/step - loss: 0.6285 - acc: 0.8003 - val_loss: 0.5074 - val_acc: 0.8338\n",
      "Epoch 3/25\n",
      "100/100 [==============================] - 133s 1s/step - loss: 0.4019 - acc: 0.8754 - val_loss: 0.3295 - val_acc: 0.8972\n",
      "Epoch 4/25\n",
      "100/100 [==============================] - 134s 1s/step - loss: 0.2979 - acc: 0.9065 - val_loss: 0.3468 - val_acc: 0.8883\n",
      "Epoch 5/25\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.1888 - acc: 0.9444 - val_loss: 0.4532 - val_acc: 0.9057\n",
      "Epoch 6/25\n",
      "100/100 [==============================] - 93s 932ms/step - loss: 0.1793 - acc: 0.9490 - val_loss: 0.2410 - val_acc: 0.9291\n",
      "Epoch 7/25\n",
      "100/100 [==============================] - 93s 927ms/step - loss: 0.1119 - acc: 0.9630 - val_loss: 0.3552 - val_acc: 0.9250\n",
      "Epoch 8/25\n",
      "100/100 [==============================] - 93s 930ms/step - loss: 0.1415 - acc: 0.9625 - val_loss: 0.3051 - val_acc: 0.9230\n",
      "Epoch 9/25\n",
      "100/100 [==============================] - 92s 922ms/step - loss: 0.0793 - acc: 0.9774 - val_loss: 0.3733 - val_acc: 0.9276\n",
      "Epoch 10/25\n",
      "100/100 [==============================] - 93s 927ms/step - loss: 0.0800 - acc: 0.9770 - val_loss: 0.7511 - val_acc: 0.8797\n",
      "Epoch 11/25\n",
      "100/100 [==============================] - 90s 902ms/step - loss: 0.0996 - acc: 0.9765 - val_loss: 0.4140 - val_acc: 0.9189\n",
      "Epoch 12/25\n",
      "100/100 [==============================] - 92s 916ms/step - loss: 0.1086 - acc: 0.9765 - val_loss: 0.5087 - val_acc: 0.8980\n",
      "Epoch 13/25\n",
      "100/100 [==============================] - 91s 914ms/step - loss: 0.0617 - acc: 0.9860 - val_loss: 0.4903 - val_acc: 0.9210\n",
      "Epoch 14/25\n",
      "100/100 [==============================] - 92s 922ms/step - loss: 0.0632 - acc: 0.9860 - val_loss: 0.5725 - val_acc: 0.9220\n",
      "Epoch 15/25\n",
      "100/100 [==============================] - 96s 955ms/step - loss: 0.0774 - acc: 0.9850 - val_loss: 0.5260 - val_acc: 0.9143\n",
      "Epoch 16/25\n",
      "100/100 [==============================] - 90s 903ms/step - loss: 0.0920 - acc: 0.9825 - val_loss: 0.5919 - val_acc: 0.9235\n",
      "Epoch 17/25\n",
      "100/100 [==============================] - 91s 910ms/step - loss: 0.0817 - acc: 0.9810 - val_loss: 0.6355 - val_acc: 0.9240\n",
      "Epoch 18/25\n",
      "100/100 [==============================] - 91s 907ms/step - loss: 0.0815 - acc: 0.9880 - val_loss: 0.8051 - val_acc: 0.9108\n",
      "Epoch 19/25\n",
      "100/100 [==============================] - 90s 900ms/step - loss: 0.1093 - acc: 0.9820 - val_loss: 0.7564 - val_acc: 0.9255\n",
      "Epoch 20/25\n",
      "100/100 [==============================] - 92s 920ms/step - loss: 0.0483 - acc: 0.9910 - val_loss: 0.7102 - val_acc: 0.9154\n",
      "Epoch 21/25\n",
      "100/100 [==============================] - 91s 911ms/step - loss: 0.0615 - acc: 0.9910 - val_loss: 0.6405 - val_acc: 0.9271\n",
      "Epoch 22/25\n",
      "100/100 [==============================] - 91s 906ms/step - loss: 0.0741 - acc: 0.9880 - val_loss: 0.6188 - val_acc: 0.9286\n",
      "Epoch 23/25\n",
      "100/100 [==============================] - 90s 896ms/step - loss: 0.0765 - acc: 0.9880 - val_loss: 0.6321 - val_acc: 0.9347\n",
      "Epoch 24/25\n",
      "100/100 [==============================] - 91s 907ms/step - loss: 0.1132 - acc: 0.9850 - val_loss: 1.4442 - val_acc: 0.8735\n",
      "Epoch 25/25\n",
      "100/100 [==============================] - 92s 915ms/step - loss: 0.0999 - acc: 0.9855 - val_loss: 0.9455 - val_acc: 0.9078\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=100,\n",
    "                              epochs=25,\n",
    "                              validation_data=validation_generator,\n",
    "                              validation_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Image_Classification_high_epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying curves of loss and accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Testing Data\n",
    "import pandas as pd\n",
    "test_filenames = os.listdir(\"E:\\\\Insofe\\\\Python Lab\\\\10_class_ic_output\\\\test1\")\n",
    "test_df = pd.DataFrame({\n",
    "    'filename': test_filenames\n",
    "})\n",
    "nb_samples = test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_image-1551435475921.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename\n",
       "0  test_image-1551435475921.jpg"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"E:\\\\Insofe\\\\Python Lab\\\\10_class_ic_output\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                              target_size=(150, 150),\n",
    "                                                              batch_size=20,\n",
    "                                                              class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Insofe\\\\Python Lab\\\\10_class_ic_output\\\\'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "batch_size=20\n",
    "predict = model.predict_generator(test_generator, steps=np.ceil(nb_samples/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99894977e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.05013576e-04, 0.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.999895, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,\n",
       "        0.000000, 0.000000, 0.000105, 0.000000]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(predict, columns=[classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>airplanes</th>\n",
       "      <th>BACKGROUND_Google</th>\n",
       "      <th>bonsai</th>\n",
       "      <th>car_side</th>\n",
       "      <th>Faces</th>\n",
       "      <th>Faces_easy</th>\n",
       "      <th>grand_piano</th>\n",
       "      <th>Leopards</th>\n",
       "      <th>Motorbikes</th>\n",
       "      <th>watch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airplanes BACKGROUND_Google bonsai car_side Faces Faces_easy grand_piano  \\\n",
       "0  0.999895               0.0    0.0      0.0   0.0        0.0         0.0   \n",
       "\n",
       "  Leopards Motorbikes watch  \n",
       "0      0.0   0.000105   0.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>airplanes</th>\n",
       "      <th>BACKGROUND_Google</th>\n",
       "      <th>bonsai</th>\n",
       "      <th>car_side</th>\n",
       "      <th>Faces</th>\n",
       "      <th>Faces_easy</th>\n",
       "      <th>grand_piano</th>\n",
       "      <th>Leopards</th>\n",
       "      <th>Motorbikes</th>\n",
       "      <th>watch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airplanes BACKGROUND_Google bonsai car_side Faces Faces_easy grand_piano  \\\n",
       "0  0.999895               0.0    0.0      0.0   0.0        0.0         0.0   \n",
       "\n",
       "  Leopards Motorbikes watch  \n",
       "0      0.0   0.000105   0.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>airplanes</th>\n",
       "      <th>BACKGROUND_Google</th>\n",
       "      <th>bonsai</th>\n",
       "      <th>car_side</th>\n",
       "      <th>Faces</th>\n",
       "      <th>Faces_easy</th>\n",
       "      <th>grand_piano</th>\n",
       "      <th>Leopards</th>\n",
       "      <th>Motorbikes</th>\n",
       "      <th>watch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airplanes BACKGROUND_Google bonsai car_side Faces Faces_easy grand_piano  \\\n",
       "0  0.999895               0.0    0.0      0.0   0.0        0.0         0.0   \n",
       "\n",
       "  Leopards Motorbikes watch  \n",
       "0      0.0   0.000105   0.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred['Max'] = y_pred.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>airplanes</th>\n",
       "      <th>BACKGROUND_Google</th>\n",
       "      <th>bonsai</th>\n",
       "      <th>car_side</th>\n",
       "      <th>Faces</th>\n",
       "      <th>Faces_easy</th>\n",
       "      <th>grand_piano</th>\n",
       "      <th>Leopards</th>\n",
       "      <th>Motorbikes</th>\n",
       "      <th>watch</th>\n",
       "      <th>Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(airplanes,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airplanes BACKGROUND_Google bonsai car_side Faces Faces_easy grand_piano  \\\n",
       "0  0.999895               0.0    0.0      0.0   0.0        0.0         0.0   \n",
       "\n",
       "  Leopards Motorbikes watch           Max  \n",
       "0      0.0   0.000105   0.0  (airplanes,)  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y_pred.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2977 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "#Transfer Learning\n",
    "# Inception was trained on 299x299x3 size images. Therefore we'll use the same dimentions for out task.\n",
    "# Nevertheless, in transfer learning, if we are importing only the convolution layers, we can resize the images to other resolutions as well\n",
    "# Inception is a fairly big network. Hence we used batch size as 64. \n",
    "# If larger memory GPUs are available, bigger batch_size could be used.\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255.,\n",
    "                                    rotation_range=20,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    zoom_range=0.2)\n",
    "train_generator = train_datagen.flow_from_directory(directory=train_dir, \n",
    "                                                    batch_size=64, \n",
    "                                                    class_mode='categorical', \n",
    "                                                    shuffle=True, \n",
    "                                                    target_size=(299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 747 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "val_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "val_generator = train_datagen.flow_from_directory(directory=validation_dir, \n",
    "                                                    batch_size=64, \n",
    "                                                    class_mode='categorical', \n",
    "                                                    shuffle=False, \n",
    "                                                    target_size=(299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.utils import to_categorical\n",
    "import cv2\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Inception architecture from keras.applications\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "def inception_tl(nb_classes, freez_wts):\n",
    "    \n",
    "    trained_model = InceptionV3(include_top=False,weights='imagenet')\n",
    "    x = trained_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    pred_inception= Dense(nb_classes,activation='softmax')(x)\n",
    "    model = Model(inputs=trained_model.input,outputs=pred_inception)\n",
    "    \n",
    "    for layer in trained_model.layers:\n",
    "        layer.trainable=(1-freez_wts)\n",
    "    \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "148/148 [==============================] - 13884s 94s/step - loss: 0.2233 - acc: 0.9379 - val_loss: 3.2038 - val_acc: 0.4195\n",
      "Epoch 2/20\n",
      "  7/148 [>.............................] - ETA: 2:55:47 - loss: 0.1438 - acc: 0.9464"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "model = inception_tl(nb_classes=10, freez_wts=False)\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "hist1 = model.fit_generator(train_generator, \n",
    "                           validation_data=val_generator, \n",
    "                           steps_per_epoch=sum(train_no)//batch_size,\n",
    "                           validation_steps=sum(valid_no)//batch_size,\n",
    "                           epochs=20).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "plt.subplot(1,2,1)\n",
    "train_loss = plt.plot(hist1['loss'], label='train loss')\n",
    "val_loss = plt.plot(hist1['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "train_loss = plt.plot(hist1['acc'], label='train acc')\n",
    "val_loss = plt.plot(hist1['val_acc'], label='val acc')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learning Rate Scheduler\n",
    "model = inception_tl(nb_classes=10, freez_wts=False)\n",
    "adam = Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 10\n",
    "init_lr = 0.001\n",
    "min_lr = 0.000001\n",
    "f = 10**(np.log10(min_lr/init_lr)/float(nb_epochs))\n",
    "\n",
    "def poly_decay(epoch):\n",
    "    ''' This function takes the current epoch as input and return the updated learning rate.\n",
    "        The learning rate is multiplied by a factor 'f' after each epoch.\n",
    "        In the first epoch, learning rate is set to 'init_lr'.\n",
    "        By the end of 'nb_epochs' the learning rate is reduced to 'min_lr' '''\n",
    "    return(init_lr*(f**epoch))\n",
    "\n",
    "# ModelCheckpoint monitors the 'val_loss' and saves the model graph and weights at the epoch with least 'val_loss'\n",
    "# 'save_weights_only'=True, saves only the weights\n",
    "# 'save_weights_only'=False, saves the weights and the graph\n",
    "chkp = ModelCheckpoint(filepath='inception_dd.h5', monitor='val_loss', save_best_only=True, save_weights_only=False, verbose=0)\n",
    "lr_schedule = LearningRateScheduler(poly_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [poly_decay(i) for i in range(nb_epochs)]\n",
    "plt.scatter(range(nb_epochs), np.log10(lr_list))\n",
    "plt.plot(np.log10(lr_list))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate in Log10 Scale')\n",
    "plt.title('Learning Rate over Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit_generator(train_generator, \n",
    "                           validation_data=val_generator, \n",
    "                           epochs=nb_epochs, \n",
    "                           callbacks=[chkp, lr_schedule]).history\n",
    "np.savez('inception_dd_history.npz', loss=hist['loss'], acc=hist['acc'], val_loss=hist['val_loss'], val_acc=hist['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "plt.subplot(1,2,1)\n",
    "train_loss = plt.plot(hist['loss'], label='train loss')\n",
    "val_loss = plt.plot(hist['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "train_loss = plt.plot(hist['acc'], label='train acc')\n",
    "val_loss = plt.plot(hist['val_acc'], label='val acc')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "# val_generator.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict_generator(generator=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_class = val_preds.argmax(axis=1)\n",
    "val_preds_df = pd.DataFrame({'image':val_generator.filenames, 'prediction':val_preds_class})\n",
    "val_preds_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrect predictions\n",
    "val_incorrect_pred = [i for i in val_preds_df.index.values if int(val_preds_df.image.values[i].split('/')[0][1])!=val_preds_df.prediction.values[i]]\n",
    "# val_incorrect_pred = [i for i,j in zip(val_preds_df.image.values,val_preds_df.prediction.values) if int(i.split('/')[0][1])!=j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_incorrect_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "plt.suptitle(\"Incorrect Predictions\", size=32)\n",
    "for ix,i in enumerate(val_incorrect_pred):\n",
    "    img = mpimg.imread(os.path.join('val',val_preds_df.image[i]))\n",
    "    plt.subplot(1,5,ix+1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Pred: '+class_map['c'+str(val_preds_df.prediction[i])]+'\\n'+'Actual: '+class_map[val_preds_df.image[i].split('/')[0]])\n",
    "    if ix==4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'img_as_float32'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-728a23dea742>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# step1 :Image transformation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mskimage\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\skimage\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0m_raise_build_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;31m# All skimage root imports go here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m     from .util.dtype import (img_as_float32,\n\u001b[0m\u001b[0;32m    168\u001b[0m                              \u001b[0mimg_as_float64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                              \u001b[0mimg_as_float\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'img_as_float32'"
     ]
    }
   ],
   "source": [
    "# step1 :Image transformation\n",
    "from scipy import ndarray\n",
    "import skimage as sk\n",
    "from skimage import transform\n",
    "from skimage import util\n",
    "\n",
    "def random_rotation(image_array: ndarray):\n",
    "    # pick a random degree of rotation between 25% on the left and 25% on the right\n",
    "    random_degree = random.uniform(-25, 25)\n",
    "    return sk.transform.rotate(image_array, random_degree)\n",
    "\n",
    "def random_noise(image_array: ndarray):\n",
    "    # add random noise to the image\n",
    "    return sk.util.random_noise(image_array)\n",
    "\n",
    "def horizontal_flip(image_array: ndarray):\n",
    "    # horizontal flip doesn't need skimage, it's easy as flipping the image array of pixels !\n",
    "    return image_array[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/a5/1d00f4bfe5ac7b48476b68f5c6bd87764a48c26e711f2bc510ea61d9c548/scikit_image-0.14.2-cp36-none-win_amd64.whl (24.6MB)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in c:\\users\\prash\\anaconda3\\lib\\site-packages (from scikit-image) (5.1.0)\n",
      "Requirement already satisfied, skipping upgrade: networkx>=1.8 in c:\\users\\prash\\anaconda3\\lib\\site-packages (from scikit-image) (2.1)\n",
      "Collecting dask[array]>=1.0.0 (from scikit-image)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/79/41d27ad703e782a422636dc8e0ce2f7624ef541b7219bd93a4af0b0d799c/dask-1.1.3-py2.py3-none-any.whl (703kB)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in c:\\users\\prash\\anaconda3\\lib\\site-packages (from scikit-image) (0.5.2)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle>=0.2.1 in c:\\users\\prash\\anaconda3\\lib\\site-packages (from scikit-image) (0.5.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in c:\\users\\prash\\anaconda3\\lib\\site-packages (from scikit-image) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.1.0 in c:\\users\\prash\\anaconda3\\lib\\site-packages (from networkx>=1.8->scikit-image) (4.3.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.0; extra == \"array\" in c:\\users\\prash\\anaconda3\\lib\\site-packages (from dask[array]>=1.0.0->scikit-image) (1.16.2)\n",
      "Requirement already satisfied, skipping upgrade: toolz>=0.7.3; extra == \"array\" in c:\\users\\prash\\anaconda3\\lib\\site-packages (from dask[array]>=1.0.0->scikit-image) (0.9.0)\n",
      "Installing collected packages: dask, scikit-image\n",
      "  Found existing installation: dask 0.17.5\n",
      "    Uninstalling dask-0.17.5:\n",
      "      Successfully uninstalled dask-0.17.5\n",
      "  Found existing installation: scikit-image 0.13.1\n",
      "    Uninstalling scikit-image-0.13.1:\n",
      "      Successfully uninstalled scikit-image-0.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\users\\\\prash\\\\anaconda3\\\\lib\\\\site-packages\\\\~kimage\\\\_shared\\\\geometry.cp36-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Cannot choose from an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-0e4b0bab12f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mnum_generated_files\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mnum_files_desired\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# random image from the folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mimage_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;31m# read image as an two dimensional array of pixels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mimage_to_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\random.py\u001b[0m in \u001b[0;36mchoice\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m    256\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot choose from an empty sequence'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Cannot choose from an empty sequence"
     ]
    }
   ],
   "source": [
    "# step 2 List all the files in a folder and read them\n",
    "import random\n",
    "import os\n",
    "\n",
    "# our folder path containing some images\n",
    "folder_path = 'E:/Insofe/Python Lab/10_categories'\n",
    "# the number of file to generate\n",
    "num_files_desired = 1000\n",
    "\n",
    "# loop on all files of the folder and build a list of files paths\n",
    "images = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "num_generated_files = 0\n",
    "while num_generated_files <= num_files_desired:\n",
    "    # random image from the folder\n",
    "    image_path = random.choice(images)\n",
    "    # read image as an two dimensional array of pixels\n",
    "    image_to_transform = sk.io.imread(image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3 — Images transformation\n",
    "# dictionary of the transformations functions we defined earlier\n",
    "available_transformations = {\n",
    "    'rotate': random_rotation,\n",
    "    'noise': random_noise,\n",
    "    'horizontal_flip': horizontal_flip\n",
    "}\n",
    "\n",
    "# random num of transformations to apply\n",
    "num_transformations_to_apply = random.randint(1, len(available_transformations))\n",
    "\n",
    "num_transformations = 0\n",
    "transformed_image = None\n",
    "while num_transformations <= num_transformations_to_apply:\n",
    "    # choose a random transformation to apply for a single image\n",
    "    key = random.choice(list(available_transformations))\n",
    "    transformed_image = available_transformations[key](image_to_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4 — Save the new images\n",
    "\n",
    "# define a name for our new file\n",
    "new_file_path = '%s/augmented_image_%s.jpg' % (folder_path, num_generated_files)\n",
    "\n",
    "# write image to the disk\n",
    "sk.io.imsave(new_file_path, transformed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
