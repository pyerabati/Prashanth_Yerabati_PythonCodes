{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAE - Denoising Auto-Encoder\n",
    "\n",
    "In this auto-encoder, we corrupt the input by slightly adding noise to it and train the network to reconstruct the original input. This can be achieved in multiple ways. \n",
    "1. Add dropout to the input. This will randomly turn off few inputs, which acts as noise. (We'll use this)\n",
    "2. Add gaussian or uniform noise to the input\n",
    "\n",
    "We'll see how classification performance varies with respect to RAW Vs. Encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "# session = tf.Session(config=config, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 781)\n",
      "(10000, 10)\n",
      "(10000, 781)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries/modules\n",
    "import numpy as np # for array operations\n",
    "from keras.models import Model, Sequential # for defining the architectures\n",
    "from keras.layers import Dense, Dropout, Input # layers for building the network\n",
    "from keras.utils import to_categorical # to_categorical does one-hot encoding\n",
    "\n",
    "# We'll use only 10,000 out of 50,000 samples for this \n",
    "tmp = np.load('cifar_pca_train.npz') # '.npz' is a dictionary which can hold many arrays\n",
    "train_data = tmp['data'][:10000]     # 'data' holds the train data\n",
    "train_labels = tmp['labels'][:10000] # 'labels' hold the corresponding labels for the above data\n",
    "\n",
    "tmp = np.load('cifar_pca_test.npz')\n",
    "test_data = tmp['data']\n",
    "test_labels = tmp['labels']\n",
    "\n",
    "\n",
    "# Converting labels into one-hot vectors for training. one-hot encoding is nothing but dummyfing\n",
    "train_labels = to_categorical(train_labels, 10) \n",
    "test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP for classifying cifar data using original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 4s - loss: 2.1546 - acc: 0.2270 - val_loss: 1.8624 - val_acc: 0.3383\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.9159 - acc: 0.3061 - val_loss: 1.8166 - val_acc: 0.3701\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.8421 - acc: 0.3433 - val_loss: 1.7608 - val_acc: 0.3819\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.7798 - acc: 0.3691 - val_loss: 1.7697 - val_acc: 0.3660\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.7501 - acc: 0.3827 - val_loss: 1.7544 - val_acc: 0.3734\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.7107 - acc: 0.3990 - val_loss: 1.7267 - val_acc: 0.3867\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.6862 - acc: 0.4063 - val_loss: 1.7223 - val_acc: 0.3914\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.6408 - acc: 0.4179 - val_loss: 1.7069 - val_acc: 0.3916\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.6135 - acc: 0.4327 - val_loss: 1.7020 - val_acc: 0.3971\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.5848 - acc: 0.4455 - val_loss: 1.6800 - val_acc: 0.4055\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.5655 - acc: 0.4544 - val_loss: 1.6790 - val_acc: 0.4112\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.5351 - acc: 0.4566 - val_loss: 1.6791 - val_acc: 0.4056\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.5177 - acc: 0.4647 - val_loss: 1.6651 - val_acc: 0.4062\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.4859 - acc: 0.4791 - val_loss: 1.6768 - val_acc: 0.4026\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.4540 - acc: 0.4875 - val_loss: 1.6606 - val_acc: 0.4152\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.4462 - acc: 0.4971 - val_loss: 1.6679 - val_acc: 0.4145\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.4103 - acc: 0.5033 - val_loss: 1.6590 - val_acc: 0.4151\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.3955 - acc: 0.5135 - val_loss: 1.6633 - val_acc: 0.4134\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.3719 - acc: 0.5243 - val_loss: 1.6671 - val_acc: 0.4113\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.3449 - acc: 0.5308 - val_loss: 1.6659 - val_acc: 0.4153\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.3437 - acc: 0.5327 - val_loss: 1.6648 - val_acc: 0.4118\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.3243 - acc: 0.5374 - val_loss: 1.6774 - val_acc: 0.4215\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2912 - acc: 0.5539 - val_loss: 1.6594 - val_acc: 0.4148\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2645 - acc: 0.5568 - val_loss: 1.6655 - val_acc: 0.4141\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.2570 - acc: 0.5564 - val_loss: 1.6810 - val_acc: 0.4185\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2433 - acc: 0.5676 - val_loss: 1.6675 - val_acc: 0.4214\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2186 - acc: 0.5753 - val_loss: 1.6827 - val_acc: 0.4164\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2037 - acc: 0.5765 - val_loss: 1.6475 - val_acc: 0.4304\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2030 - acc: 0.5799 - val_loss: 1.6647 - val_acc: 0.4222\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1776 - acc: 0.5884 - val_loss: 1.6823 - val_acc: 0.4251\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1590 - acc: 0.6042 - val_loss: 1.6782 - val_acc: 0.4250\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1505 - acc: 0.6027 - val_loss: 1.6805 - val_acc: 0.4232\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1239 - acc: 0.6081 - val_loss: 1.6752 - val_acc: 0.4266\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.1063 - acc: 0.6160 - val_loss: 1.6902 - val_acc: 0.4235\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1107 - acc: 0.6180 - val_loss: 1.6731 - val_acc: 0.4244\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0685 - acc: 0.6293 - val_loss: 1.6792 - val_acc: 0.4275\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0867 - acc: 0.6218 - val_loss: 1.6873 - val_acc: 0.4225\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0658 - acc: 0.6331 - val_loss: 1.6781 - val_acc: 0.4222\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0354 - acc: 0.6384 - val_loss: 1.6898 - val_acc: 0.4281\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0412 - acc: 0.6427 - val_loss: 1.7043 - val_acc: 0.4261\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0187 - acc: 0.6469 - val_loss: 1.7224 - val_acc: 0.4215\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0151 - acc: 0.6433 - val_loss: 1.7091 - val_acc: 0.4285\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0140 - acc: 0.6462 - val_loss: 1.6981 - val_acc: 0.4286\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9944 - acc: 0.6539 - val_loss: 1.7273 - val_acc: 0.4259\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9786 - acc: 0.6642 - val_loss: 1.7175 - val_acc: 0.4218\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9447 - acc: 0.6714 - val_loss: 1.7238 - val_acc: 0.4223\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9426 - acc: 0.6759 - val_loss: 1.7341 - val_acc: 0.4249\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9343 - acc: 0.6819 - val_loss: 1.7115 - val_acc: 0.4319\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9080 - acc: 0.6839 - val_loss: 1.7284 - val_acc: 0.4330\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9259 - acc: 0.6820 - val_loss: 1.7295 - val_acc: 0.4288\n"
     ]
    }
   ],
   "source": [
    "# training a two hidden layer MLP for classification task\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dropout(0.2, input_shape=(781,)))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(10, activation='softmax'))\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "nb_epoch = 50      # number of epochs\n",
    "batch_size = 32    # batch size\n",
    "history = mlp.fit(train_data, train_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6812 - mean_squared_error: 0.6812 - val_loss: 0.3500 - val_mean_squared_error: 0.3500\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5708 - mean_squared_error: 0.5708 - val_loss: 0.2852 - val_mean_squared_error: 0.2852\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5476 - mean_squared_error: 0.5476 - val_loss: 0.2601 - val_mean_squared_error: 0.2601\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5385 - mean_squared_error: 0.5385 - val_loss: 0.2485 - val_mean_squared_error: 0.2485\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5316 - mean_squared_error: 0.5316 - val_loss: 0.2373 - val_mean_squared_error: 0.2373\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5302 - mean_squared_error: 0.5302 - val_loss: 0.2328 - val_mean_squared_error: 0.2328\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5278 - mean_squared_error: 0.5278 - val_loss: 0.2299 - val_mean_squared_error: 0.2299\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5202 - mean_squared_error: 0.5202 - val_loss: 0.2242 - val_mean_squared_error: 0.2242\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5170 - mean_squared_error: 0.5170 - val_loss: 0.2199 - val_mean_squared_error: 0.2199\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5164 - mean_squared_error: 0.5164 - val_loss: 0.2195 - val_mean_squared_error: 0.2195\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5208 - mean_squared_error: 0.5208 - val_loss: 0.2176 - val_mean_squared_error: 0.2176\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5118 - mean_squared_error: 0.5118 - val_loss: 0.2136 - val_mean_squared_error: 0.2136\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5156 - mean_squared_error: 0.5156 - val_loss: 0.2157 - val_mean_squared_error: 0.2157\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5120 - mean_squared_error: 0.5120 - val_loss: 0.2118 - val_mean_squared_error: 0.2118\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5127 - mean_squared_error: 0.5127 - val_loss: 0.2103 - val_mean_squared_error: 0.2103\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5102 - mean_squared_error: 0.5102 - val_loss: 0.2104 - val_mean_squared_error: 0.2104\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5124 - mean_squared_error: 0.5124 - val_loss: 0.2098 - val_mean_squared_error: 0.2098\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5125 - mean_squared_error: 0.5125 - val_loss: 0.2080 - val_mean_squared_error: 0.2080\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5050 - mean_squared_error: 0.5050 - val_loss: 0.2066 - val_mean_squared_error: 0.2066\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5045 - mean_squared_error: 0.5045 - val_loss: 0.2048 - val_mean_squared_error: 0.2048\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5066 - mean_squared_error: 0.5066 - val_loss: 0.2052 - val_mean_squared_error: 0.2052\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5080 - mean_squared_error: 0.5080 - val_loss: 0.2050 - val_mean_squared_error: 0.2050\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5091 - mean_squared_error: 0.5091 - val_loss: 0.2025 - val_mean_squared_error: 0.2025\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5065 - mean_squared_error: 0.5065 - val_loss: 0.2058 - val_mean_squared_error: 0.2058\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5077 - mean_squared_error: 0.5077 - val_loss: 0.2017 - val_mean_squared_error: 0.2017\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5034 - mean_squared_error: 0.5034 - val_loss: 0.2035 - val_mean_squared_error: 0.2035\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5049 - mean_squared_error: 0.5049 - val_loss: 0.2036 - val_mean_squared_error: 0.2036\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5014 - mean_squared_error: 0.5014 - val_loss: 0.2016 - val_mean_squared_error: 0.2016\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5051 - mean_squared_error: 0.5051 - val_loss: 0.2005 - val_mean_squared_error: 0.2005\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5011 - mean_squared_error: 0.5011 - val_loss: 0.1997 - val_mean_squared_error: 0.1997\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5008 - mean_squared_error: 0.5008 - val_loss: 0.2018 - val_mean_squared_error: 0.2018\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4999 - mean_squared_error: 0.4999 - val_loss: 0.2000 - val_mean_squared_error: 0.2000\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5018 - mean_squared_error: 0.5018 - val_loss: 0.1999 - val_mean_squared_error: 0.1999\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4980 - mean_squared_error: 0.4980 - val_loss: 0.1990 - val_mean_squared_error: 0.1990\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4991 - mean_squared_error: 0.4991 - val_loss: 0.1988 - val_mean_squared_error: 0.1988\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4987 - mean_squared_error: 0.4987 - val_loss: 0.1976 - val_mean_squared_error: 0.1976\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4976 - mean_squared_error: 0.4976 - val_loss: 0.1986 - val_mean_squared_error: 0.1986\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4966 - mean_squared_error: 0.4966 - val_loss: 0.1960 - val_mean_squared_error: 0.1960\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4951 - mean_squared_error: 0.4951 - val_loss: 0.2006 - val_mean_squared_error: 0.2006\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4950 - mean_squared_error: 0.4950 - val_loss: 0.1987 - val_mean_squared_error: 0.1987\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4951 - mean_squared_error: 0.4951 - val_loss: 0.2007 - val_mean_squared_error: 0.2007\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4928 - mean_squared_error: 0.4928 - val_loss: 0.1991 - val_mean_squared_error: 0.1991\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4951 - mean_squared_error: 0.4951 - val_loss: 0.1996 - val_mean_squared_error: 0.1996\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4943 - mean_squared_error: 0.4943 - val_loss: 0.1977 - val_mean_squared_error: 0.1977\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4920 - mean_squared_error: 0.4920 - val_loss: 0.2005 - val_mean_squared_error: 0.2005\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4937 - mean_squared_error: 0.4937 - val_loss: 0.2031 - val_mean_squared_error: 0.2031\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4911 - mean_squared_error: 0.4911 - val_loss: 0.1987 - val_mean_squared_error: 0.1987\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4895 - mean_squared_error: 0.4895 - val_loss: 0.2027 - val_mean_squared_error: 0.2027\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4892 - mean_squared_error: 0.4892 - val_loss: 0.1996 - val_mean_squared_error: 0.1996\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4919 - mean_squared_error: 0.4919 - val_loss: 0.2000 - val_mean_squared_error: 0.2000\n"
     ]
    }
   ],
   "source": [
    "# Building an auto-encoder architecture with 'Model' function. This is a bit defferent from 'Sequential' type.\n",
    "# For this, we need to create a series of layers connected together. \n",
    "# Once we have the connections in place, we can use model to define the architecture.\n",
    "# To 'Model', we simply mention the first layer and the last layer.\n",
    "\n",
    "nb_epoch = 50      # number of epochs\n",
    "batch_size = 32    # batch size\n",
    "\n",
    "input_img = Input(shape=(781,))\n",
    "crrpt_img = Dropout(0.5)(input_img)\n",
    "encoded = Dense(1000, activation='sigmoid')(crrpt_img)\n",
    "decoded = Dense(781, activation='linear')(encoded)\n",
    "\n",
    "autoencoder = Model(input_img,decoded)\n",
    "autoencoder.compile(optimizer='adam',\n",
    "                    loss='mean_squared_error')\n",
    "\n",
    "history = autoencoder.fit(train_data, train_data,  \n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(test_data, test_data))\n",
    "\n",
    "autoencoder.save('DAE_l1_model.h5') # save the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For predicting encoded output, we can define a model which starts with input layer and ends with encoded layer\n",
    "# Since these layers are already trained, we can directly predict the encoded values\n",
    "\n",
    "encoder = Model(input_img,encoded)\n",
    "htrain_data = encoder.predict(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP for classifying cifar data using encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "adam = Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10000, 1000), (10000, 10))\n",
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s - loss: 2.0458 - acc: 0.2645 - val_loss: 1.8417 - val_acc: 0.3386\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.8226 - acc: 0.3504 - val_loss: 1.7803 - val_acc: 0.3662\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.7520 - acc: 0.3779 - val_loss: 1.7306 - val_acc: 0.3822\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.7034 - acc: 0.3979 - val_loss: 1.7332 - val_acc: 0.3840\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.6570 - acc: 0.4159 - val_loss: 1.7385 - val_acc: 0.3847\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.6137 - acc: 0.4310 - val_loss: 1.6896 - val_acc: 0.3938\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.5799 - acc: 0.4465 - val_loss: 1.6942 - val_acc: 0.3964\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.5386 - acc: 0.4622 - val_loss: 1.6447 - val_acc: 0.4174\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.5048 - acc: 0.4618 - val_loss: 1.6501 - val_acc: 0.4198\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.4584 - acc: 0.4867 - val_loss: 1.6402 - val_acc: 0.4176\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.4250 - acc: 0.4964 - val_loss: 1.6087 - val_acc: 0.4316\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.3852 - acc: 0.5178 - val_loss: 1.5805 - val_acc: 0.4402\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.3638 - acc: 0.5159 - val_loss: 1.5731 - val_acc: 0.4440\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.3337 - acc: 0.5321 - val_loss: 1.5744 - val_acc: 0.4428\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2842 - acc: 0.5460 - val_loss: 1.5532 - val_acc: 0.4496\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2638 - acc: 0.5548 - val_loss: 1.5700 - val_acc: 0.4390\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2241 - acc: 0.5727 - val_loss: 1.5412 - val_acc: 0.4536\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1910 - acc: 0.5780 - val_loss: 1.5579 - val_acc: 0.4559\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1617 - acc: 0.5912 - val_loss: 1.5380 - val_acc: 0.4582\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1252 - acc: 0.6086 - val_loss: 1.5615 - val_acc: 0.4450\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0839 - acc: 0.6261 - val_loss: 1.5274 - val_acc: 0.4622\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0612 - acc: 0.6300 - val_loss: 1.5382 - val_acc: 0.4605\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0118 - acc: 0.6490 - val_loss: 1.5605 - val_acc: 0.4566\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9968 - acc: 0.6520 - val_loss: 1.5487 - val_acc: 0.4603\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9607 - acc: 0.6638 - val_loss: 1.5449 - val_acc: 0.4588\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9352 - acc: 0.6753 - val_loss: 1.5458 - val_acc: 0.4653\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.8904 - acc: 0.6944 - val_loss: 1.5516 - val_acc: 0.4673\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.8723 - acc: 0.6963 - val_loss: 1.5421 - val_acc: 0.4722\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.8553 - acc: 0.7060 - val_loss: 1.5706 - val_acc: 0.4606\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.8215 - acc: 0.7183 - val_loss: 1.5472 - val_acc: 0.4642\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.7995 - acc: 0.7258 - val_loss: 1.5365 - val_acc: 0.4707\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.7662 - acc: 0.7403 - val_loss: 1.5651 - val_acc: 0.4662\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.7438 - acc: 0.7387 - val_loss: 1.5625 - val_acc: 0.4690\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.7140 - acc: 0.7551 - val_loss: 1.5673 - val_acc: 0.4669\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6890 - acc: 0.7696 - val_loss: 1.5661 - val_acc: 0.4694\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6730 - acc: 0.7749 - val_loss: 1.5720 - val_acc: 0.4694\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6458 - acc: 0.7808 - val_loss: 1.5906 - val_acc: 0.4679\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6245 - acc: 0.7882 - val_loss: 1.5689 - val_acc: 0.4714\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5999 - acc: 0.7960 - val_loss: 1.6138 - val_acc: 0.4701\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5965 - acc: 0.7965 - val_loss: 1.5817 - val_acc: 0.4743\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5653 - acc: 0.8093 - val_loss: 1.5862 - val_acc: 0.4731\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5465 - acc: 0.8180 - val_loss: 1.5876 - val_acc: 0.4708\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5513 - acc: 0.8123 - val_loss: 1.6136 - val_acc: 0.4667\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5169 - acc: 0.8254 - val_loss: 1.5900 - val_acc: 0.4754\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5089 - acc: 0.8302 - val_loss: 1.6528 - val_acc: 0.4679\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4888 - acc: 0.8346 - val_loss: 1.6172 - val_acc: 0.4732\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4744 - acc: 0.8406 - val_loss: 1.6775 - val_acc: 0.4684\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4762 - acc: 0.8368 - val_loss: 1.6430 - val_acc: 0.4710\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4518 - acc: 0.8449 - val_loss: 1.6464 - val_acc: 0.4748\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4394 - acc: 0.8519 - val_loss: 1.6689 - val_acc: 0.4716\n"
     ]
    }
   ],
   "source": [
    "# training an MLP on autoencoder features\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dropout(0.2, input_shape=(1000,)))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(10, activation='softmax'))\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "htest_data = encoder.predict(test_data)\n",
    "print(htest_data.shape, test_labels.shape)\n",
    "history = mlp.fit(htrain_data[:10000], train_labels[:10000],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    validation_data=(htest_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
