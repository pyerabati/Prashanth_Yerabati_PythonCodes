{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A string is defined within quotes (single or double)\n",
    "#join() method returns a string in which string elements of sequence (B) have been joined by 'A' Seperator.\n",
    "A = 'Insofe'\n",
    "B = 'Batch53'\n",
    "C = 'X123'\n",
    "D = '123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BInsofeaInsofetInsofecInsofehInsofe5Insofe3\n",
      "B a t c h 5 3\n",
      "InsofeBatch53\n"
     ]
    }
   ],
   "source": [
    "print(A.join(B))\n",
    "print(\" \".join(B))\n",
    "print(A+B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insofe\n",
      "INSOFE\n",
      "-1\n",
      "0\n",
      "BInsofeaInsofetInsofecInsofehInsofe5Insofe3\n",
      "B a t c h 5 3\n",
      "InsofeBatch53\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Lets perform some string manipulations on the string\n",
    "print(A.lower())\n",
    "print(A.upper())\n",
    "print(A.find('a'))\n",
    "print(A.count('a'))\n",
    "print(A.join(B))\n",
    "print(\" \".join(B))\n",
    "print(A+B)\n",
    "print(A.isdigit())\n",
    "print(D.isdigit())\n",
    "print(C.isdigit())\n",
    "print(C.isalnum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "??A.find() # Doc string help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsofe\n",
      "Insofe\n",
      "Insofe\n",
      "Isf\n",
      "efosnI\n",
      "I\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7f74c4619564>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Seq[Start:End:Step]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'shr'\u001b[0m \u001b[1;31m#Strings are immutablea\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "print(A.strip('I')) ##Python String strip() method returns a copy of the string with both leading and trailing characters removed\n",
    "print(A)\n",
    "print(A[::1]) #Seq[Start:End:Step]\n",
    "print(A[::2]) #Seq[Start:End:Step]\n",
    "print(A[::-1]) #Seq[Start:End:Step]\n",
    "print(A[0])\n",
    "A[0] = 'shr' #Strings are immutablea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '''\n",
    "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
    "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
    "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
    "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
    "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', \"don't\", 'train', 'for', 'Leatherhead,', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes.', 'It', 'was', 'a', 'perfect', 'day,', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens.', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots,', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth.', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged.', 'My', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap,', 'his', 'arms', 'folded,', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes,', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast,', 'buried', 'in', 'the', 'deepest', 'thought.', 'Suddenly,', 'however,', 'he', 'started,', 'tapped', 'me', 'on', 'the', 'shoulder,', 'and', 'pointed', 'over', 'the', 'meadows.']\n"
     ]
    }
   ],
   "source": [
    "# ##Some string manipulations- Splitting the paragraph by spaces\n",
    "Str_list = string.split() # split() method returns a list of strings after breaking the given string by the specified separator.\n",
    "print(Str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\nAt Waterloo we were fortunate in catching a don't train for Leatherhead\", ' where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \\nIt was a perfect day', ' with a bright sun and a few fleecy clouds in the heavens. \\nThe trees and wayside hedges were just throwing out their first green shoots', ' and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \\nMy companion sat in the front of the trap', ' his arms folded', ' his hat pulled down over his eyes', ' and his chin sunk upon his breast', ' buried in the deepest thought. \\nSuddenly', ' however', ' he started', ' tapped me on the shoulder', ' and pointed over the meadows.\\n']\n"
     ]
    }
   ],
   "source": [
    "## Splitting the paragraph by a punctuation marks\n",
    "Str_list = string.split(\",\") # #split() method returns a list of strings after breaking the given string by the specified separator.\n",
    "print(Str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading text file\n",
    "import os # The OS module in Python provides a way of using operating system dependent functionality. \n",
    "# Return information identifying the current operating system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Insofe\\\\Python Lab'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "os.chdir(path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb', '.ipynb_checkpoints', '2010 Report to Congress on White House.csv', '20181111_Batch53_CSE7212c_Python_Review_Activity.ipynb', '20181215_Batch53_CSE7305c_Hierarchichal.ipynb', '20181215_Batch53_CSE7305c_Kmeans.ipynb', '20181216_Batch53_CS7305c_Lab02_ARules-checkpoint.ipynb', '20181216_Batch53_CS7305c_Lab02_ARules.ipynb', '20181216_Batch53_CSE7305c_DT (1).ipynb', '20181216_Batch53_CSE7305c_DT-checkpoint.ipynb', '20181216_Batch53_CSE7305c_DT.ipynb', '20181222_Batch53_CSE7305c_CF.pdf', '20181222_Batch53_CSE7305c_KNN.pdf', '20181222_Batch53_CSE7305c_KNN_Activity.ipynb', '20181222_Batch53_CSE_7305c_Lab_Collaborative_Filtering_v2 (1).ipynb', '20181222_Batch53_CSE_7305c_Lab_Collaborative_Filtering_v2.ipynb', '20181229_Batch53_CSE7305c_Lect04_SVM.pdf', '20181229_Batch53_CSE7305c_SVR.ipynb', '20181230_Batch53_CSE7305c_Day04_SVM_Fill_the_blanks.ipynb', '20181230_Batch53_CSE7305c_Day05_SVM_Filled.ipynb', '20181230_Batch53_CSE_7305c_Boosting_Classfication_Filled.ipynb', '20190105_Batch 53_CSE 7321c_Lab01_Activity.pdf', '20190113_Batch 53_CSE 7124c_Lab01_LSA.pdf', '20190113_Batch 53_CSE 7124c_Lab01_LSA.R', '20190113_Batch+53_CSE+7124c_Lab01_Text_Preprocessing.html', '20190113_Batch+53_CSE+7124c_Lab01_TFIDF.html', '20190113_Batch_53_CSE_7124c_Lab01_Regular_expressions.ipynb', '20190119_Batch 53_CSE 7124c_Lab02_Activity.pdf', '20190119_Batch 53_CSE7124c_SentimentAnalysis_Code.ipynb', '20190119_Batch 53_CSE7124c_TextClassification_Code.ipynb', 'Africa.csv', 'Airfares.csv', 'Association_Rules.ipynb', 'Association_rules_practice.ipynb', 'BackOrders.csv', 'BackOrdersData_Description.txt', 'bank.csv', 'bank_data.csv', 'bank_data_KNN_Imputed.csv', 'bank_data_KNN_Imputed1.csv', 'bill_authentication.csv', 'BinaryClassification_Perceptron.ipynb', 'BinaryClassification_Perceptron_Regression.ipynb', 'Coffee Chain.xlsx', 'CustomerData.csv', 'Customer_Bank Details_MV.csv', 'Customer_Demographics_MV_DOB.csv', 'Customer_satisfaction_classification.ipynb', 'Cute2_Partb.ipynb', 'Cute3_bankuptcy.ipynb', 'cute3_bankurptcy_FS.ipynb', 'cute3_bankurptcy_KNN_imputation.ipynb', 'cute3_bankurptcy_smote_all_basic_models.ipynb', 'Cute3_FS.ipynb', 'Cute3_Problem_Basic_ModelBuilding_Logistic_SVC_DT_KNN_RFC (3).ipynb', 'cute_python-checkpoint.ipynb', 'cute_python.ipynb', 'Data.csv', 'Datasets', 'data_test.csv', 'data_train.csv', 'day1_lab1.ipynb', 'day1_lab1_practice.ipynb', 'day1_practice1.ipynb', 'deliveries.csv', 'Demo_PreProcessingFn.ipynb', 'diabetes-checkpoint.machine_learing_1', 'diabetes.machine_learing_1', 'Diabetes_Data.csv', 'diabetes_machine_learning_1-checkpoint.ipynb', 'doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt', 'doc5.txt', 'doc6.txt', 'ende_cleaned.json', 'ex1.ipynb', 'feature_selector.py', 'Global Superstore Orders 2016.xlsx', 'hist_age.png', 'house_pricedata_description.txt', 'House_price_detaild.ipynb', 'house_price_start.ipynb', 'house_price_test.csv', 'house_price_train.csv', 'IPL.ipynb', 'ipltest2-checkpoint.ipynb', 'ipltest2.ipynb', 'Ipl_practice-checkpoint.ipynb', 'Ipl_practice.ipynb', 'ipl_test-checkpoint.ipynb', 'ipl_test.ipynb', 'jester_items.tsv', 'jester_ratings.csv', 'Joins.xlsx', 'KCLT.csv', 'Kmeans_Jokes_ratings.ipynb', 'KMeans_Titanic_survival.ipynb', 'KPHL.csv', 'LinearRegressionAssignment (1).ipynb', 'mariral_vs_pur_stack.png', 'matches2008-2016.csv', 'matches2008-2016_1.csv', 'matches2017.csv', 'matches2017_1.csv', 'merge1.csv', 'merge2.csv', 'merge3.csv', 'MovieRatings.csv', 'mtcars.csv', 'Mulitple_Linear_regression_boston_example.ipynb', 'negative.txt', 'new_bank.csv', 'new_customer.csv', 'Office City.xlsx', 'one.csv', 'Perceptron_GridSearch.ipynb', 'petrol_consumption.csv', 'pima-indians-diabetes.csv', 'pima_indians_diabetes.csv', 'positive.txt', 'Problem Statement.docx', 'Problem Statement.txt', 'purchase_fre_job.png', 'pur_dayofMonth_bar.png', 'pur_dayofweek_bar.png', 'pur_fre_month_bar.png', 'pur_fre_pout_bar.png', 'Python Codes', 'Python datasets', 'python_work.ipynb', 'Random_Forest_Regression_classification_practice.ipynb', 'Regression_Multi-layer_Perceptron.ipynb', 'sample_submission.csv', 'sherlock.txt', 'SMSSpamCollection.csv', 'submission1.csv', 'submission2.csv', 'submission3.csv', 'submission4.csv', 'SVM.ipynb', 'SVM_Bank_IRIS.ipynb', 'test-checkpoint.ipynb', 'test.ipynb', 'test.py.ipynb', 'test_sample.csv', 'titanic.txt', 'titanic_data.csv', 'train_sample.csv', 'Transactions.csv', 'UniversalBank.csv', 'Untitled.ipynb', 'visualize_weather.py', 'weather-checkpoint.ipynb', 'weather.ipynb', 'Week10_day2_Boosing_Adaboost.ipynb', 'Week10_day2_RandomForest_Bagging.ipynb', 'Week10_day2_stacking.ipynb', 'week10_day2_stacking_LR_DT_SVM.ipynb', 'Week11_Day1_Perceptron_Regression.ipynb', 'week3_111118.ipynb', 'week4_day1_111618.ipynb', 'week4_day1_111718.txt', 'week4_day1_111718_practice.ipynb', 'week4_day1_2.1_111718_practice.ipynb', 'week4_day2.1_111718-checkpoint.ipynb', 'week4_day2.1_111718.ipynb', 'week4_day2_D111618_Assignment_Practice.ipynb', 'week8_day1.1_d12152018_hierarchy-checkpoint.machine_learning', 'week8_day1.1_d12152018_hierarchy.machine_learning.ipynb', 'Week8_day1_Association_Rules.ipynb', 'week8_day1_d12152018_diabetes_machine_learning_1-checkpoint.ipynb', 'week8_day1_d12152018_diabetes_machine_learning_1.ipynb', 'week8_day1_Heirarchy_mtcars.ipynb', 'Week8_day1_Kmeans_Adjusted_Rand_Score.ipynb', 'Week8_day1_Kmeans_Adjusted_Rand_score.ipynb1', 'Week8_day2_Decisioin_Tree.ipynb', 'Week8_day2_Machinglearing_DecisionTree-checkpoint.ipynb', 'Week8_day2_Machinglearing_DecisionTree.ipynb', 'week9_day1_Knn_Consolidation_CF.ipynb', 'week9_day1_KNN_neighbors.ipynb', 'week9_day1_python_lab_knn-checkpoint.ipynb', 'week9_day1_python_lab_knn.ipynb', 'wunderground_parser.py', 'wunderground_scraper.py', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(os.getcwd()))  # verify what files are there in the path as shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once we have our required text file(s)/folder(s) in the working directory we use the following function\n",
    "with open('sherlock.txt', 'r') as x:\n",
    "    string1 = x.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \\nIt was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \\nThe trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \\nMy companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \\nSuddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that our sample text is ready, let us perform the following stepsÂ¶\n",
    "#Sentence Tokenizing\n",
    "#Word Tokenizing\n",
    "#Stop Word Removal\n",
    "#Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\Prashanth Yerabati/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Prashanth Yerabati\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-f941505744ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Sentence Tokenization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 836\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\Prashanth Yerabati/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Prashanth Yerabati\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
