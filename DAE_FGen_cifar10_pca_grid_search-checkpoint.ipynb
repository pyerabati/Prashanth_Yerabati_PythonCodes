{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAE - Denoising Auto-Encoder\n",
    "\n",
    "In this auto-encoder, we corrupt the input by slightly adding noise to it and train the network to reconstruct the original input. This can be achieved in multiple ways. \n",
    "1. Add dropout to the input. This will randomly turn off few inputs, which acts as noise. (We'll use this)\n",
    "2. Add gaussian or uniform noise to the input\n",
    "\n",
    "We'll see how classification performance varies with respect to RAW Vs. Encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-b4ce2269e7a9>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-b4ce2269e7a9>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    session = tf.Session(config=config, ...)\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 781)\n",
      "(10000, 10)\n",
      "(10000, 781)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries/modules\n",
    "import numpy as np # for array operations\n",
    "from keras.models import Model, Sequential # for defining the architectures\n",
    "from keras.layers import Dense, Dropout, Input # layers for building the network\n",
    "from keras.utils import to_categorical # to_categorical does one-hot encoding\n",
    "\n",
    "# We'll use only 10,000 out of 50,000 samples for this \n",
    "tmp = np.load('cifar_pca_train.npz') # '.npz' is a dictionary which can hold many arrays\n",
    "train_data = tmp['data'][:10000]     # 'data' holds the train data\n",
    "train_labels = tmp['labels'][:10000] # 'labels' hold the corresponding labels for the above data\n",
    "\n",
    "tmp = np.load('cifar_pca_test.npz')\n",
    "test_data = tmp['data']\n",
    "test_labels = tmp['labels']\n",
    "\n",
    "\n",
    "# Converting labels into one-hot vectors for training. one-hot encoding is nothing but dummyfing\n",
    "train_labels = to_categorical(train_labels, 10) \n",
    "test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP for classifying cifar data using original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 39s 4ms/step - loss: 2.1465 - acc: 0.2251 - val_loss: 1.8685 - val_acc: 0.3435\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 43s 4ms/step - loss: 1.9154 - acc: 0.3131 - val_loss: 1.7969 - val_acc: 0.3709\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 38s 4ms/step - loss: 1.8484 - acc: 0.3406 - val_loss: 1.7748 - val_acc: 0.3702\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 36s 4ms/step - loss: 1.7951 - acc: 0.3649 - val_loss: 1.7488 - val_acc: 0.3761\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 35s 4ms/step - loss: 1.7572 - acc: 0.3805 - val_loss: 1.7351 - val_acc: 0.3844\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 36s 4ms/step - loss: 1.7256 - acc: 0.3927 - val_loss: 1.7596 - val_acc: 0.3747\n",
      "Epoch 7/50\n",
      "  192/10000 [..............................] - ETA: 29s - loss: 1.6166 - acc: 0.4167"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-57e447fda2cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                     validation_data=(test_data, test_labels))\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2719\u001b[0m                     \u001b[1;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[1;32m-> 2721\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2693\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2694\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training a two hidden layer MLP for classification task\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dropout(0.2, input_shape=(781,)))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Multi class -Soft Max and Cross entropy\n",
    "# for Binary Class, sigmoid, Binary crsoo entropy\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# Optimizer decies how you compute the gradient and how you update the weights\n",
    "# MSE is the metric for regression and accuracy is metric for classification, We want Keras to calculate accuracy\n",
    "# after every epoch\n",
    "\n",
    "nb_epoch = 50      # number of epochs\n",
    "batch_size = 32    # batch size\n",
    "history = mlp.fit(train_data, train_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6812 - mean_squared_error: 0.6812 - val_loss: 0.3500 - val_mean_squared_error: 0.3500\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5708 - mean_squared_error: 0.5708 - val_loss: 0.2852 - val_mean_squared_error: 0.2852\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5476 - mean_squared_error: 0.5476 - val_loss: 0.2601 - val_mean_squared_error: 0.2601\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5385 - mean_squared_error: 0.5385 - val_loss: 0.2485 - val_mean_squared_error: 0.2485\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5316 - mean_squared_error: 0.5316 - val_loss: 0.2373 - val_mean_squared_error: 0.2373\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5302 - mean_squared_error: 0.5302 - val_loss: 0.2328 - val_mean_squared_error: 0.2328\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5278 - mean_squared_error: 0.5278 - val_loss: 0.2299 - val_mean_squared_error: 0.2299\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5202 - mean_squared_error: 0.5202 - val_loss: 0.2242 - val_mean_squared_error: 0.2242\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5170 - mean_squared_error: 0.5170 - val_loss: 0.2199 - val_mean_squared_error: 0.2199\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5164 - mean_squared_error: 0.5164 - val_loss: 0.2195 - val_mean_squared_error: 0.2195\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5208 - mean_squared_error: 0.5208 - val_loss: 0.2176 - val_mean_squared_error: 0.2176\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5118 - mean_squared_error: 0.5118 - val_loss: 0.2136 - val_mean_squared_error: 0.2136\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5156 - mean_squared_error: 0.5156 - val_loss: 0.2157 - val_mean_squared_error: 0.2157\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5120 - mean_squared_error: 0.5120 - val_loss: 0.2118 - val_mean_squared_error: 0.2118\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5127 - mean_squared_error: 0.5127 - val_loss: 0.2103 - val_mean_squared_error: 0.2103\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5102 - mean_squared_error: 0.5102 - val_loss: 0.2104 - val_mean_squared_error: 0.2104\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5124 - mean_squared_error: 0.5124 - val_loss: 0.2098 - val_mean_squared_error: 0.2098\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5125 - mean_squared_error: 0.5125 - val_loss: 0.2080 - val_mean_squared_error: 0.2080\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5050 - mean_squared_error: 0.5050 - val_loss: 0.2066 - val_mean_squared_error: 0.2066\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5045 - mean_squared_error: 0.5045 - val_loss: 0.2048 - val_mean_squared_error: 0.2048\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5066 - mean_squared_error: 0.5066 - val_loss: 0.2052 - val_mean_squared_error: 0.2052\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5080 - mean_squared_error: 0.5080 - val_loss: 0.2050 - val_mean_squared_error: 0.2050\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5091 - mean_squared_error: 0.5091 - val_loss: 0.2025 - val_mean_squared_error: 0.2025\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5065 - mean_squared_error: 0.5065 - val_loss: 0.2058 - val_mean_squared_error: 0.2058\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5077 - mean_squared_error: 0.5077 - val_loss: 0.2017 - val_mean_squared_error: 0.2017\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5034 - mean_squared_error: 0.5034 - val_loss: 0.2035 - val_mean_squared_error: 0.2035\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5049 - mean_squared_error: 0.5049 - val_loss: 0.2036 - val_mean_squared_error: 0.2036\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5014 - mean_squared_error: 0.5014 - val_loss: 0.2016 - val_mean_squared_error: 0.2016\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5051 - mean_squared_error: 0.5051 - val_loss: 0.2005 - val_mean_squared_error: 0.2005\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5011 - mean_squared_error: 0.5011 - val_loss: 0.1997 - val_mean_squared_error: 0.1997\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5008 - mean_squared_error: 0.5008 - val_loss: 0.2018 - val_mean_squared_error: 0.2018\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4999 - mean_squared_error: 0.4999 - val_loss: 0.2000 - val_mean_squared_error: 0.2000\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5018 - mean_squared_error: 0.5018 - val_loss: 0.1999 - val_mean_squared_error: 0.1999\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4980 - mean_squared_error: 0.4980 - val_loss: 0.1990 - val_mean_squared_error: 0.1990\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4991 - mean_squared_error: 0.4991 - val_loss: 0.1988 - val_mean_squared_error: 0.1988\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4987 - mean_squared_error: 0.4987 - val_loss: 0.1976 - val_mean_squared_error: 0.1976\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4976 - mean_squared_error: 0.4976 - val_loss: 0.1986 - val_mean_squared_error: 0.1986\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4966 - mean_squared_error: 0.4966 - val_loss: 0.1960 - val_mean_squared_error: 0.1960\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4951 - mean_squared_error: 0.4951 - val_loss: 0.2006 - val_mean_squared_error: 0.2006\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4950 - mean_squared_error: 0.4950 - val_loss: 0.1987 - val_mean_squared_error: 0.1987\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4951 - mean_squared_error: 0.4951 - val_loss: 0.2007 - val_mean_squared_error: 0.2007\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4928 - mean_squared_error: 0.4928 - val_loss: 0.1991 - val_mean_squared_error: 0.1991\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4951 - mean_squared_error: 0.4951 - val_loss: 0.1996 - val_mean_squared_error: 0.1996\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4943 - mean_squared_error: 0.4943 - val_loss: 0.1977 - val_mean_squared_error: 0.1977\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4920 - mean_squared_error: 0.4920 - val_loss: 0.2005 - val_mean_squared_error: 0.2005\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4937 - mean_squared_error: 0.4937 - val_loss: 0.2031 - val_mean_squared_error: 0.2031\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4911 - mean_squared_error: 0.4911 - val_loss: 0.1987 - val_mean_squared_error: 0.1987\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4895 - mean_squared_error: 0.4895 - val_loss: 0.2027 - val_mean_squared_error: 0.2027\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4892 - mean_squared_error: 0.4892 - val_loss: 0.1996 - val_mean_squared_error: 0.1996\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4919 - mean_squared_error: 0.4919 - val_loss: 0.2000 - val_mean_squared_error: 0.2000\n"
     ]
    }
   ],
   "source": [
    "# Building an auto-encoder architecture with 'Model' function. This is a bit defferent from 'Sequential' type.\n",
    "# For this, we need to create a series of layers connected together. \n",
    "# Once we have the connections in place, we can use model to define the architecture.\n",
    "# To 'Model', we simply mention the first layer and the last layer.\n",
    "\n",
    "nb_epoch = 50      # number of epochs\n",
    "batch_size = 32    # batch size\n",
    "#Dee series of indivudal layers\n",
    "input_img = Input(shape=(781,)) # input to the Input_img layer original features\n",
    "crrpt_img = Dropout(0.5)(input_img) # input to the crrpt_img is input_img\n",
    "encoded = Dense(1000, activation='sigmoid')(crrpt_img) # INput to the econded layer is Input_img\n",
    "decoded = Dense(781, activation='linear')(encoded) # input to the decoded layer is encoded  \n",
    "\n",
    "autoencoder = Model(input_img,decoded)\n",
    "autoencoder.compile(optimizer='adam',\n",
    "                    loss='mean_squared_error')\n",
    "\n",
    "history = autoencoder.fit(train_data, train_data,  \n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(test_data, test_data))\n",
    "\n",
    "autoencoder.save('DAE_l1_model.h5') # save the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For predicting encoded output, we can define a model which starts with input layer and ends with encoded layer\n",
    "# Since these layers are already trained, we can directly predict the encoded values\n",
    "\n",
    "encoder = Model(input_img,encoded)\n",
    "htrain_data = encoder.predict(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP for classifying cifar data using encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "adam = Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10000, 1000), (10000, 10))\n",
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s - loss: 2.0458 - acc: 0.2645 - val_loss: 1.8417 - val_acc: 0.3386\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.8226 - acc: 0.3504 - val_loss: 1.7803 - val_acc: 0.3662\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.7520 - acc: 0.3779 - val_loss: 1.7306 - val_acc: 0.3822\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.7034 - acc: 0.3979 - val_loss: 1.7332 - val_acc: 0.3840\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.6570 - acc: 0.4159 - val_loss: 1.7385 - val_acc: 0.3847\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.6137 - acc: 0.4310 - val_loss: 1.6896 - val_acc: 0.3938\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.5799 - acc: 0.4465 - val_loss: 1.6942 - val_acc: 0.3964\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.5386 - acc: 0.4622 - val_loss: 1.6447 - val_acc: 0.4174\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.5048 - acc: 0.4618 - val_loss: 1.6501 - val_acc: 0.4198\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.4584 - acc: 0.4867 - val_loss: 1.6402 - val_acc: 0.4176\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.4250 - acc: 0.4964 - val_loss: 1.6087 - val_acc: 0.4316\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.3852 - acc: 0.5178 - val_loss: 1.5805 - val_acc: 0.4402\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.3638 - acc: 0.5159 - val_loss: 1.5731 - val_acc: 0.4440\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.3337 - acc: 0.5321 - val_loss: 1.5744 - val_acc: 0.4428\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2842 - acc: 0.5460 - val_loss: 1.5532 - val_acc: 0.4496\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2638 - acc: 0.5548 - val_loss: 1.5700 - val_acc: 0.4390\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2241 - acc: 0.5727 - val_loss: 1.5412 - val_acc: 0.4536\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1910 - acc: 0.5780 - val_loss: 1.5579 - val_acc: 0.4559\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1617 - acc: 0.5912 - val_loss: 1.5380 - val_acc: 0.4582\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1252 - acc: 0.6086 - val_loss: 1.5615 - val_acc: 0.4450\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0839 - acc: 0.6261 - val_loss: 1.5274 - val_acc: 0.4622\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0612 - acc: 0.6300 - val_loss: 1.5382 - val_acc: 0.4605\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0118 - acc: 0.6490 - val_loss: 1.5605 - val_acc: 0.4566\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9968 - acc: 0.6520 - val_loss: 1.5487 - val_acc: 0.4603\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9607 - acc: 0.6638 - val_loss: 1.5449 - val_acc: 0.4588\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9352 - acc: 0.6753 - val_loss: 1.5458 - val_acc: 0.4653\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.8904 - acc: 0.6944 - val_loss: 1.5516 - val_acc: 0.4673\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.8723 - acc: 0.6963 - val_loss: 1.5421 - val_acc: 0.4722\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.8553 - acc: 0.7060 - val_loss: 1.5706 - val_acc: 0.4606\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.8215 - acc: 0.7183 - val_loss: 1.5472 - val_acc: 0.4642\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.7995 - acc: 0.7258 - val_loss: 1.5365 - val_acc: 0.4707\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.7662 - acc: 0.7403 - val_loss: 1.5651 - val_acc: 0.4662\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.7438 - acc: 0.7387 - val_loss: 1.5625 - val_acc: 0.4690\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.7140 - acc: 0.7551 - val_loss: 1.5673 - val_acc: 0.4669\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6890 - acc: 0.7696 - val_loss: 1.5661 - val_acc: 0.4694\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6730 - acc: 0.7749 - val_loss: 1.5720 - val_acc: 0.4694\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6458 - acc: 0.7808 - val_loss: 1.5906 - val_acc: 0.4679\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6245 - acc: 0.7882 - val_loss: 1.5689 - val_acc: 0.4714\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5999 - acc: 0.7960 - val_loss: 1.6138 - val_acc: 0.4701\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5965 - acc: 0.7965 - val_loss: 1.5817 - val_acc: 0.4743\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5653 - acc: 0.8093 - val_loss: 1.5862 - val_acc: 0.4731\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5465 - acc: 0.8180 - val_loss: 1.5876 - val_acc: 0.4708\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5513 - acc: 0.8123 - val_loss: 1.6136 - val_acc: 0.4667\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5169 - acc: 0.8254 - val_loss: 1.5900 - val_acc: 0.4754\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5089 - acc: 0.8302 - val_loss: 1.6528 - val_acc: 0.4679\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4888 - acc: 0.8346 - val_loss: 1.6172 - val_acc: 0.4732\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4744 - acc: 0.8406 - val_loss: 1.6775 - val_acc: 0.4684\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4762 - acc: 0.8368 - val_loss: 1.6430 - val_acc: 0.4710\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4518 - acc: 0.8449 - val_loss: 1.6464 - val_acc: 0.4748\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4394 - acc: 0.8519 - val_loss: 1.6689 - val_acc: 0.4716\n"
     ]
    }
   ],
   "source": [
    "# training an MLP on autoencoder features\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dropout(0.2, input_shape=(1000,)))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(10, activation='softmax'))\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "htest_data = encoder.predict(test_data)\n",
    "print(htest_data.shape, test_labels.shape)\n",
    "history = mlp.fit(htrain_data[:10000], train_labels[:10000],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    validation_data=(htest_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000_0.5_0.001\n"
     ]
    }
   ],
   "source": [
    "nodes = '1000'\n",
    "dp = '0.5' \n",
    "lrt = '0.001'\n",
    "ll = [nodes, dp, lrt]\n",
    "path = '_'.join(ll)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(nodes, dp, lrt):\n",
    "    path = '_'.join([str(nodes), str(dp), str(lrt)])\n",
    "    nb_epoch = 50      # number of epochs\n",
    "    batch_size = 32    # batch size\n",
    "\n",
    "    input_img = Input(shape=(781,))\n",
    "    crrpt_img = Dropout(dp)(input_img)\n",
    "    encoded = Dense(nodes, activation='sigmoid')(crrpt_img)\n",
    "    decoded = Dense(781, activation='linear')(encoded)\n",
    "\n",
    "    adam = Adam(lr=lrt)\n",
    "    autoencoder = Model(input_img,decoded)\n",
    "    autoencoder.compile(optimizer=adam,\n",
    "                        loss='mean_squared_error')\n",
    "\n",
    "    autoencoder.fit(train_data, train_data, validation_data=(test_data, test_data))\n",
    "    autoencoder.save(path+'_DAE.h5') # save the model weights\n",
    "    \n",
    "    encoder = Model(input_img,encoded)\n",
    "    htrain_data = encoder.predict(train_data)\n",
    "    \n",
    "    mlp = Sequential()\n",
    "    mlp.add(Dropout(0.2, input_shape=(1000,)))\n",
    "    mlp.add(Dense(1000, activation='sigmoid'))\n",
    "    mlp.add(Dropout(0.5))\n",
    "    mlp.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    mlp.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    htest_data = encoder.predict(test_data)\n",
    "    print(htest_data.shape, test_labels.shape)\n",
    "    mlp.fit(htrain_data[:10000], train_labels[:10000],\n",
    "                        epochs=nb_epoch)\n",
    "    \n",
    "    test_loss, test_acc = mlp.evaluate(htest_data, test_labels)\n",
    "    return(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = [500, 1000, 1500]\n",
    "dp1 = [0.3,0.4,0.5]\n",
    "lr1 = [0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "l1 = list(itertools.product(n1, dp1, lr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_values = []\n",
    "for v1, v2, v3 in l1:\n",
    "    temp = run(nodes=v1, dp=v2, lrt=v3)\n",
    "    test_acc_values.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list(l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1500</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1500</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1500</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1500</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1       2\n",
       "0    500  0.3  0.0010\n",
       "1    500  0.3  0.0001\n",
       "2    500  0.4  0.0010\n",
       "3    500  0.4  0.0001\n",
       "4    500  0.5  0.0010\n",
       "5    500  0.5  0.0001\n",
       "6   1000  0.3  0.0010\n",
       "7   1000  0.3  0.0001\n",
       "8   1000  0.4  0.0010\n",
       "9   1000  0.4  0.0001\n",
       "10  1000  0.5  0.0010\n",
       "11  1000  0.5  0.0001\n",
       "12  1500  0.3  0.0010\n",
       "13  1500  0.3  0.0001\n",
       "14  1500  0.4  0.0010\n",
       "15  1500  0.4  0.0001\n",
       "16  1500  0.5  0.0010\n",
       "17  1500  0.5  0.0001"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
