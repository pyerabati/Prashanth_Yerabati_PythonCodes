{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prash\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D,Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Insofe\\\\Python Lab'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link for the dataset:\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n",
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#GLOVE_DIR = 'glove.6B/'\n",
    "glove_data = 'glove.6B.100d.txt'\n",
    "TEXT_DATA_DIR = '20news_18828/'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "nb_epochs = 50\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(glove_data,encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18828 texts.\n"
     ]
    }
   ],
   "source": [
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "#                 f = open(fpath)\n",
    "                if sys.version_info < (3,):\n",
    "                    f = open(fpath)\n",
    "                else:\n",
    "                    f = open(fpath, encoding='latin-1')\n",
    "                t = f.read()\n",
    "                i = t.find('\\n\\n')  # skip header\n",
    "                if 0 < i:\n",
    "                    t = t[i:]\n",
    "                texts.append(t)\n",
    "                f.close()\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 172668 unique tokens.\n",
      "Shape of data tensor: (18828, 1000)\n",
      "Shape of label tensor: (18828, 20)\n"
     ]
    }
   ],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n"
     ]
    }
   ],
   "source": [
    "TEXT_DATA_DIR = '20news_18828/'\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - GLOVE Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "Train on 14121 samples, validate on 4707 samples\n",
      "Epoch 1/50\n",
      "14121/14121 [==============================] - 411s 29ms/step - loss: 2.7323 - acc: 0.1333 - val_loss: 2.3436 - val_acc: 0.2384\n",
      "Epoch 2/50\n",
      "14121/14121 [==============================] - 439s 31ms/step - loss: 2.2328 - acc: 0.2403 - val_loss: 2.0433 - val_acc: 0.3023\n",
      "Epoch 3/50\n",
      "14121/14121 [==============================] - 403s 29ms/step - loss: 2.1251 - acc: 0.2843 - val_loss: 1.9540 - val_acc: 0.3535\n",
      "Epoch 4/50\n",
      "14121/14121 [==============================] - 416s 29ms/step - loss: 1.8648 - acc: 0.3644 - val_loss: 1.6169 - val_acc: 0.4631\n",
      "Epoch 5/50\n",
      "14121/14121 [==============================] - 405s 29ms/step - loss: 1.6480 - acc: 0.4494 - val_loss: 1.4897 - val_acc: 0.4929\n",
      "Epoch 6/50\n",
      "14121/14121 [==============================] - 431s 31ms/step - loss: 1.4857 - acc: 0.4952 - val_loss: 1.2859 - val_acc: 0.5681\n",
      "Epoch 7/50\n",
      "14121/14121 [==============================] - 349s 25ms/step - loss: 1.3142 - acc: 0.5534 - val_loss: 1.1945 - val_acc: 0.5804\n",
      "Epoch 8/50\n",
      "14121/14121 [==============================] - 247s 17ms/step - loss: 1.4393 - acc: 0.5415 - val_loss: 1.4465 - val_acc: 0.5434\n",
      "Epoch 9/50\n",
      "14121/14121 [==============================] - 246s 17ms/step - loss: 1.3738 - acc: 0.5464 - val_loss: 1.1488 - val_acc: 0.6297\n",
      "Epoch 10/50\n",
      "14121/14121 [==============================] - 243s 17ms/step - loss: 1.1893 - acc: 0.6104 - val_loss: 1.0541 - val_acc: 0.6543\n",
      "Epoch 11/50\n",
      "14121/14121 [==============================] - 228s 16ms/step - loss: 1.1419 - acc: 0.6255 - val_loss: 1.0344 - val_acc: 0.6645\n",
      "Epoch 12/50\n",
      "14121/14121 [==============================] - 228s 16ms/step - loss: 1.0758 - acc: 0.6457 - val_loss: 0.9936 - val_acc: 0.6771\n",
      "Epoch 13/50\n",
      "14121/14121 [==============================] - 231s 16ms/step - loss: 1.0216 - acc: 0.6609 - val_loss: 0.9561 - val_acc: 0.6879\n",
      "Epoch 14/50\n",
      "14121/14121 [==============================] - 231s 16ms/step - loss: 0.9770 - acc: 0.6797 - val_loss: 0.9138 - val_acc: 0.6981\n",
      "Epoch 15/50\n",
      "14121/14121 [==============================] - 231s 16ms/step - loss: 0.9440 - acc: 0.6901 - val_loss: 0.9045 - val_acc: 0.7130\n",
      "Epoch 16/50\n",
      "14121/14121 [==============================] - 227s 16ms/step - loss: 0.9472 - acc: 0.6911 - val_loss: 0.8915 - val_acc: 0.7094\n",
      "Epoch 17/50\n",
      "14121/14121 [==============================] - 229s 16ms/step - loss: 0.8945 - acc: 0.7070 - val_loss: 0.8595 - val_acc: 0.7228\n",
      "Epoch 18/50\n",
      "14121/14121 [==============================] - 229s 16ms/step - loss: 0.8642 - acc: 0.7157 - val_loss: 0.8591 - val_acc: 0.7266\n",
      "Epoch 19/50\n",
      "14121/14121 [==============================] - 230s 16ms/step - loss: 0.8169 - acc: 0.7332 - val_loss: 0.8394 - val_acc: 0.7330\n",
      "Epoch 20/50\n",
      "14121/14121 [==============================] - 228s 16ms/step - loss: 0.8058 - acc: 0.7361 - val_loss: 0.8424 - val_acc: 0.7325\n",
      "Epoch 21/50\n",
      "14121/14121 [==============================] - 230s 16ms/step - loss: 0.7924 - acc: 0.7465 - val_loss: 0.8008 - val_acc: 0.7451\n",
      "Epoch 22/50\n",
      "14121/14121 [==============================] - 229s 16ms/step - loss: 0.7735 - acc: 0.7509 - val_loss: 0.7880 - val_acc: 0.7529\n",
      "Epoch 23/50\n",
      "14121/14121 [==============================] - 229s 16ms/step - loss: 0.7334 - acc: 0.7616 - val_loss: 0.7854 - val_acc: 0.7557\n",
      "Epoch 24/50\n",
      "14121/14121 [==============================] - 235s 17ms/step - loss: 0.7429 - acc: 0.7600 - val_loss: 0.7830 - val_acc: 0.7572\n",
      "Epoch 25/50\n",
      "14121/14121 [==============================] - 228s 16ms/step - loss: 0.6935 - acc: 0.7761 - val_loss: 0.7681 - val_acc: 0.7674\n",
      "Epoch 26/50\n",
      "14121/14121 [==============================] - 227s 16ms/step - loss: 0.6784 - acc: 0.7812 - val_loss: 0.7583 - val_acc: 0.7665\n",
      "Epoch 27/50\n",
      "14121/14121 [==============================] - 227s 16ms/step - loss: 0.6516 - acc: 0.7933 - val_loss: 0.7601 - val_acc: 0.7667\n",
      "Epoch 28/50\n",
      "14121/14121 [==============================] - 227s 16ms/step - loss: 0.6408 - acc: 0.7965 - val_loss: 0.7310 - val_acc: 0.7752\n",
      "Epoch 29/50\n",
      "14121/14121 [==============================] - 227s 16ms/step - loss: 0.6120 - acc: 0.8041 - val_loss: 0.7415 - val_acc: 0.7742\n",
      "Epoch 30/50\n",
      "14121/14121 [==============================] - 229s 16ms/step - loss: 0.6027 - acc: 0.8077 - val_loss: 0.7292 - val_acc: 0.7814\n",
      "Epoch 31/50\n",
      "14121/14121 [==============================] - 229s 16ms/step - loss: 0.5796 - acc: 0.8165 - val_loss: 0.7216 - val_acc: 0.7863\n",
      "Epoch 32/50\n",
      "14121/14121 [==============================] - 229s 16ms/step - loss: 0.5566 - acc: 0.8230 - val_loss: 0.7025 - val_acc: 0.7922\n",
      "Epoch 33/50\n",
      "14121/14121 [==============================] - 227s 16ms/step - loss: 0.5617 - acc: 0.8191 - val_loss: 0.7110 - val_acc: 0.7924\n",
      "Epoch 34/50\n",
      "14121/14121 [==============================] - 229s 16ms/step - loss: 0.5578 - acc: 0.8195 - val_loss: 0.7112 - val_acc: 0.7918\n",
      "Epoch 35/50\n",
      "14121/14121 [==============================] - 226s 16ms/step - loss: 0.5504 - acc: 0.8255 - val_loss: 0.7198 - val_acc: 0.7916\n",
      "Epoch 36/50\n",
      "14121/14121 [==============================] - 227s 16ms/step - loss: 0.5650 - acc: 0.8206 - val_loss: 0.7501 - val_acc: 0.7791\n",
      "Epoch 37/50\n",
      "14121/14121 [==============================] - 226s 16ms/step - loss: 0.5339 - acc: 0.8300 - val_loss: 0.7249 - val_acc: 0.7878\n",
      "Epoch 38/50\n",
      "14121/14121 [==============================] - 231s 16ms/step - loss: 0.4969 - acc: 0.8421 - val_loss: 0.7249 - val_acc: 0.7890\n",
      "Epoch 39/50\n",
      "14121/14121 [==============================] - 229s 16ms/step - loss: 0.5048 - acc: 0.8373 - val_loss: 0.7066 - val_acc: 0.7971\n",
      "Epoch 40/50\n",
      "14121/14121 [==============================] - 232s 16ms/step - loss: 0.4710 - acc: 0.8454 - val_loss: 0.6894 - val_acc: 0.8001\n",
      "Epoch 41/50\n",
      "14121/14121 [==============================] - 232s 16ms/step - loss: 0.5041 - acc: 0.8364 - val_loss: 0.7452 - val_acc: 0.7786\n",
      "Epoch 42/50\n",
      "14121/14121 [==============================] - 232s 16ms/step - loss: 0.5069 - acc: 0.8371 - val_loss: 0.7217 - val_acc: 0.7952\n",
      "Epoch 43/50\n",
      "14121/14121 [==============================] - 231s 16ms/step - loss: 0.4525 - acc: 0.8539 - val_loss: 0.6933 - val_acc: 0.8067\n",
      "Epoch 44/50\n",
      "14121/14121 [==============================] - 233s 16ms/step - loss: 0.4539 - acc: 0.8519 - val_loss: 0.6885 - val_acc: 0.8031\n",
      "Epoch 45/50\n",
      "14121/14121 [==============================] - 228s 16ms/step - loss: 0.4456 - acc: 0.8550 - val_loss: 0.6893 - val_acc: 0.8058\n",
      "Epoch 46/50\n",
      "14121/14121 [==============================] - 227s 16ms/step - loss: 0.4056 - acc: 0.8683 - val_loss: 0.7011 - val_acc: 0.8101\n",
      "Epoch 47/50\n",
      "14121/14121 [==============================] - 227s 16ms/step - loss: 0.4076 - acc: 0.8708 - val_loss: 0.6992 - val_acc: 0.8065\n",
      "Epoch 48/50\n",
      "14121/14121 [==============================] - 229s 16ms/step - loss: 0.3995 - acc: 0.8688 - val_loss: 0.6900 - val_acc: 0.8099\n",
      "Epoch 49/50\n",
      "14121/14121 [==============================] - 228s 16ms/step - loss: 0.3920 - acc: 0.8735 - val_loss: 0.7141 - val_acc: 0.8035\n",
      "Epoch 50/50\n",
      "14121/14121 [==============================] - 233s 17ms/step - loss: 0.3680 - acc: 0.8786 - val_loss: 0.7226 - val_acc: 0.8043\n"
     ]
    }
   ],
   "source": [
    "print('Training model.')\n",
    "\n",
    "from keras.layers import LSTM\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "z = Dropout(0.2)(embedded_sequences)\n",
    "z = LSTM(128)(z)\n",
    "z = Dropout(0.4)(z)\n",
    "preds_lstm = Dense(20, activation='softmax')(z)\n",
    "\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "adam = Adam(lr=0.001)\n",
    "model_lstm = Model(sequence_input, preds_lstm)\n",
    "model_lstm.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])\n",
    "\n",
    "model_lstm_hist = model_lstm.fit(x_train, y_train,\n",
    "                  batch_size=128,\n",
    "                  epochs=nb_epochs,\n",
    "                  validation_data=(x_test, y_test)).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D-CNN - GLOVE Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "Train on 15063 samples, validate on 3765 samples\n",
      "Epoch 1/50\n",
      "15063/15063 [==============================] - 2s - loss: 2.3406 - acc: 0.2369 - val_loss: 1.8085 - val_acc: 0.3880\n",
      "Epoch 2/50\n",
      "15063/15063 [==============================] - 2s - loss: 1.3878 - acc: 0.5302 - val_loss: 1.1940 - val_acc: 0.5997\n",
      "Epoch 3/50\n",
      "15063/15063 [==============================] - 2s - loss: 1.1086 - acc: 0.6241 - val_loss: 1.1211 - val_acc: 0.6218\n",
      "Epoch 4/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.9185 - acc: 0.6930 - val_loss: 0.9874 - val_acc: 0.6637\n",
      "Epoch 5/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.7551 - acc: 0.7489 - val_loss: 0.8764 - val_acc: 0.7150\n",
      "Epoch 6/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.6309 - acc: 0.7908 - val_loss: 0.8812 - val_acc: 0.7102\n",
      "Epoch 7/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.5078 - acc: 0.8334 - val_loss: 0.8623 - val_acc: 0.7325\n",
      "Epoch 8/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.3932 - acc: 0.8759 - val_loss: 0.9215 - val_acc: 0.7222\n",
      "Epoch 9/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.2885 - acc: 0.9116 - val_loss: 0.9261 - val_acc: 0.7307\n",
      "Epoch 10/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.2040 - acc: 0.9406 - val_loss: 1.0421 - val_acc: 0.7320\n",
      "Epoch 11/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.1641 - acc: 0.9527 - val_loss: 1.0173 - val_acc: 0.7487\n",
      "Epoch 12/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.1052 - acc: 0.9722 - val_loss: 1.0050 - val_acc: 0.7618\n",
      "Epoch 13/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0667 - acc: 0.9867 - val_loss: 1.0600 - val_acc: 0.7591\n",
      "Epoch 14/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0471 - acc: 0.9908 - val_loss: 1.0780 - val_acc: 0.7625\n",
      "Epoch 15/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0775 - acc: 0.9786 - val_loss: 1.1551 - val_acc: 0.7612\n",
      "Epoch 16/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0457 - acc: 0.9884 - val_loss: 1.1403 - val_acc: 0.7692\n",
      "Epoch 17/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0264 - acc: 0.9951 - val_loss: 1.2676 - val_acc: 0.7615\n",
      "Epoch 18/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0303 - acc: 0.9934 - val_loss: 1.3660 - val_acc: 0.7487\n",
      "Epoch 19/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0838 - acc: 0.9756 - val_loss: 1.1973 - val_acc: 0.7501\n",
      "Epoch 20/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0761 - acc: 0.9771 - val_loss: 1.1719 - val_acc: 0.7742\n",
      "Epoch 21/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0432 - acc: 0.9891 - val_loss: 1.2616 - val_acc: 0.7689\n",
      "Epoch 22/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0449 - acc: 0.9883 - val_loss: 1.2964 - val_acc: 0.7546\n",
      "Epoch 23/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0307 - acc: 0.9936 - val_loss: 1.3117 - val_acc: 0.7652\n",
      "Epoch 24/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0270 - acc: 0.9934 - val_loss: 1.3770 - val_acc: 0.7655\n",
      "Epoch 25/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0202 - acc: 0.9956 - val_loss: 1.3682 - val_acc: 0.7618\n",
      "Epoch 26/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0221 - acc: 0.9943 - val_loss: 1.4130 - val_acc: 0.7599\n",
      "Epoch 27/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0768 - acc: 0.9746 - val_loss: 1.4246 - val_acc: 0.7389\n",
      "Epoch 28/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0500 - acc: 0.9845 - val_loss: 1.4159 - val_acc: 0.7527\n",
      "Epoch 29/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0528 - acc: 0.9834 - val_loss: 1.4261 - val_acc: 0.7461\n",
      "Epoch 30/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0984 - acc: 0.9687 - val_loss: 1.3269 - val_acc: 0.7633\n",
      "Epoch 31/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0247 - acc: 0.9941 - val_loss: 1.4356 - val_acc: 0.7607\n",
      "Epoch 32/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0189 - acc: 0.9954 - val_loss: 1.3874 - val_acc: 0.7705\n",
      "Epoch 33/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0209 - acc: 0.9945 - val_loss: 1.4407 - val_acc: 0.7620\n",
      "Epoch 34/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0342 - acc: 0.9903 - val_loss: 1.4220 - val_acc: 0.7641\n",
      "Epoch 35/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0203 - acc: 0.9948 - val_loss: 1.3555 - val_acc: 0.7705\n",
      "Epoch 36/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0203 - acc: 0.9944 - val_loss: 1.4020 - val_acc: 0.7652\n",
      "Epoch 37/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0163 - acc: 0.9958 - val_loss: 1.4274 - val_acc: 0.7718\n",
      "Epoch 38/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0261 - acc: 0.9930 - val_loss: 1.4669 - val_acc: 0.7641\n",
      "Epoch 39/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0280 - acc: 0.9924 - val_loss: 1.5392 - val_acc: 0.7572\n",
      "Epoch 40/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0899 - acc: 0.9708 - val_loss: 1.4969 - val_acc: 0.7325\n",
      "Epoch 41/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0845 - acc: 0.9734 - val_loss: 1.4057 - val_acc: 0.7583\n",
      "Epoch 42/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0335 - acc: 0.9902 - val_loss: 1.4430 - val_acc: 0.7625\n",
      "Epoch 43/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0325 - acc: 0.9903 - val_loss: 1.4020 - val_acc: 0.7710\n",
      "Epoch 44/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0142 - acc: 0.9969 - val_loss: 1.4135 - val_acc: 0.7857\n",
      "Epoch 45/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0169 - acc: 0.9957 - val_loss: 1.5115 - val_acc: 0.7700\n",
      "Epoch 46/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0246 - acc: 0.9926 - val_loss: 1.4454 - val_acc: 0.7687\n",
      "Epoch 47/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0494 - acc: 0.9849 - val_loss: 1.5097 - val_acc: 0.7482\n",
      "Epoch 48/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0363 - acc: 0.9894 - val_loss: 1.4592 - val_acc: 0.7710\n",
      "Epoch 49/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0161 - acc: 0.9959 - val_loss: 1.5269 - val_acc: 0.7580\n",
      "Epoch 50/50\n",
      "15063/15063 [==============================] - 2s - loss: 0.0166 - acc: 0.9958 - val_loss: 1.4366 - val_acc: 0.7766\n"
     ]
    }
   ],
   "source": [
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "adam = Adam(lr=0.001)\n",
    "model1 = Model(sequence_input, preds)\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])\n",
    "\n",
    "model1_hist = model1.fit(x_train, y_train,\n",
    "              batch_size=128,\n",
    "              epochs=nb_epochs,\n",
    "              validation_data=(x_val, y_val)).history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
