{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAE - Denoising Auto-Encoder\n",
    "\n",
    "In this auto-encoder, we corrupt the input by slightly adding noise to it and train the network to reconstruct the original input. This can be achieved in multiple ways. \n",
    "1. Add dropout to the input. This will randomly turn off few inputs, which acts as noise. (We'll use this)\n",
    "2. Add gaussian or uniform noise to the input\n",
    "\n",
    "We'll see how classification performance varies with respect to RAW Vs. Encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-4-36b848ba5d90>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-36b848ba5d90>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    session = tf.Session(config=config, ...)\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 781)\n",
      "(10000, 10)\n",
      "(10000, 781)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries/modules\n",
    "import numpy as np # for array operations\n",
    "from keras.models import Model, Sequential # for defining the architectures\n",
    "from keras.layers import Dense, Dropout, Input # layers for building the network\n",
    "from keras.utils import to_categorical # to_categorical does one-hot encoding\n",
    "\n",
    "# We'll use only 10,000 out of 50,000 samples for this \n",
    "tmp = np.load('cifar_pca_train.npz') # '.npz' is a dictionary which can hold many arrays\n",
    "train_data = tmp['data'][:10000]     # 'data' holds the train data\n",
    "train_labels = tmp['labels'][:10000] # 'labels' hold the corresponding labels for the above data\n",
    "\n",
    "tmp = np.load('cifar_pca_test.npz')\n",
    "test_data = tmp['data']\n",
    "test_labels = tmp['labels']\n",
    "\n",
    "\n",
    "# Converting labels into one-hot vectors for training. one-hot encoding is nothing but dummyfing\n",
    "train_labels = to_categorical(train_labels, 10) \n",
    "test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP for classifying cifar data using original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 41s 4ms/step - loss: 2.1502 - acc: 0.2232 - val_loss: 1.8601 - val_acc: 0.3446\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 45s 4ms/step - loss: 1.9239 - acc: 0.3065 - val_loss: 1.8068 - val_acc: 0.3674\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 43s 4ms/step - loss: 1.8372 - acc: 0.3426 - val_loss: 1.7814 - val_acc: 0.3630\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 44s 4ms/step - loss: 1.7875 - acc: 0.3687 - val_loss: 1.7386 - val_acc: 0.3847\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 43s 4ms/step - loss: 1.7458 - acc: 0.3822 - val_loss: 1.7720 - val_acc: 0.3633\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 43s 4ms/step - loss: 1.7164 - acc: 0.3999 - val_loss: 1.7289 - val_acc: 0.3915\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 43s 4ms/step - loss: 1.6801 - acc: 0.4080 - val_loss: 1.7084 - val_acc: 0.3951\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 42s 4ms/step - loss: 1.6421 - acc: 0.4233 - val_loss: 1.7091 - val_acc: 0.3995\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 42s 4ms/step - loss: 1.6147 - acc: 0.4319 - val_loss: 1.6855 - val_acc: 0.4023\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 42s 4ms/step - loss: 1.5815 - acc: 0.4422 - val_loss: 1.7027 - val_acc: 0.3952\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 42s 4ms/step - loss: 1.5572 - acc: 0.4559 - val_loss: 1.6989 - val_acc: 0.3985\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 42s 4ms/step - loss: 1.5436 - acc: 0.4637 - val_loss: 1.6718 - val_acc: 0.4099\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 44s 4ms/step - loss: 1.5170 - acc: 0.4651 - val_loss: 1.6915 - val_acc: 0.4008\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 43s 4ms/step - loss: 1.4758 - acc: 0.4852 - val_loss: 1.6722 - val_acc: 0.4138\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 44s 4ms/step - loss: 1.4675 - acc: 0.4841 - val_loss: 1.6774 - val_acc: 0.4076\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 36s 4ms/step - loss: 1.4486 - acc: 0.4960 - val_loss: 1.6574 - val_acc: 0.4157\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 35s 3ms/step - loss: 1.4272 - acc: 0.4989 - val_loss: 1.6672 - val_acc: 0.4129\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 35s 3ms/step - loss: 1.3990 - acc: 0.5097 - val_loss: 1.6581 - val_acc: 0.4146\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 1.3701 - acc: 0.5218 - val_loss: 1.6749 - val_acc: 0.4158\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 35s 3ms/step - loss: 1.3595 - acc: 0.5291 - val_loss: 1.6774 - val_acc: 0.4083\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 35s 3ms/step - loss: 1.3321 - acc: 0.5360 - val_loss: 1.6809 - val_acc: 0.4164\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 44s 4ms/step - loss: 1.3086 - acc: 0.5397 - val_loss: 1.6709 - val_acc: 0.4146\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 39s 4ms/step - loss: 1.2829 - acc: 0.5539 - val_loss: 1.6661 - val_acc: 0.4175\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 37s 4ms/step - loss: 1.2825 - acc: 0.5463 - val_loss: 1.6715 - val_acc: 0.4119\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 38s 4ms/step - loss: 1.2644 - acc: 0.5536 - val_loss: 1.6810 - val_acc: 0.4151\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 38s 4ms/step - loss: 1.2469 - acc: 0.5673 - val_loss: 1.6565 - val_acc: 0.4238\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 38s 4ms/step - loss: 1.2283 - acc: 0.5723 - val_loss: 1.6664 - val_acc: 0.4232\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 42s 4ms/step - loss: 1.2310 - acc: 0.5723 - val_loss: 1.6613 - val_acc: 0.4244\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 44s 4ms/step - loss: 1.1825 - acc: 0.5866 - val_loss: 1.6648 - val_acc: 0.4274\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 45s 5ms/step - loss: 1.1617 - acc: 0.5975 - val_loss: 1.6917 - val_acc: 0.4146\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 43s 4ms/step - loss: 1.1434 - acc: 0.5970 - val_loss: 1.6757 - val_acc: 0.4241\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 238s 24ms/step - loss: 1.1280 - acc: 0.6058 - val_loss: 1.6544 - val_acc: 0.4246\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 87s 9ms/step - loss: 1.1201 - acc: 0.6132 - val_loss: 1.6805 - val_acc: 0.4187\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 78s 8ms/step - loss: 1.1181 - acc: 0.6169 - val_loss: 1.6783 - val_acc: 0.4204\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 7799s 780ms/step - loss: 1.0972 - acc: 0.6225 - val_loss: 1.6604 - val_acc: 0.4252\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 97s 10ms/step - loss: 1.0635 - acc: 0.6284 - val_loss: 1.7014 - val_acc: 0.4172\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 80s 8ms/step - loss: 1.0633 - acc: 0.6237 - val_loss: 1.7219 - val_acc: 0.4205\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 93s 9ms/step - loss: 1.0558 - acc: 0.6380 - val_loss: 1.6977 - val_acc: 0.4231\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 88s 9ms/step - loss: 1.0464 - acc: 0.6357 - val_loss: 1.6938 - val_acc: 0.4301\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 98s 10ms/step - loss: 1.0241 - acc: 0.6371 - val_loss: 1.7002 - val_acc: 0.4353\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 197s 20ms/step - loss: 1.0130 - acc: 0.6423 - val_loss: 1.7176 - val_acc: 0.4279\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 77s 8ms/step - loss: 1.0007 - acc: 0.6599 - val_loss: 1.7110 - val_acc: 0.4171\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 91s 9ms/step - loss: 1.0000 - acc: 0.6515 - val_loss: 1.6938 - val_acc: 0.4261\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 95s 9ms/step - loss: 0.9819 - acc: 0.6637 - val_loss: 1.7080 - val_acc: 0.4221\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 105s 11ms/step - loss: 0.9548 - acc: 0.6718 - val_loss: 1.7079 - val_acc: 0.4192\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 92s 9ms/step - loss: 0.9492 - acc: 0.6680 - val_loss: 1.7103 - val_acc: 0.4285\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 103s 10ms/step - loss: 0.9496 - acc: 0.6713 - val_loss: 1.7078 - val_acc: 0.4290\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 201s 20ms/step - loss: 0.9164 - acc: 0.6801 - val_loss: 1.6945 - val_acc: 0.4245\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 91s 9ms/step - loss: 0.9139 - acc: 0.6886 - val_loss: 1.7116 - val_acc: 0.4286\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 99s 10ms/step - loss: 0.9103 - acc: 0.6865 - val_loss: 1.7305 - val_acc: 0.4281\n"
     ]
    }
   ],
   "source": [
    "# training a two hidden layer MLP for classification task\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dropout(0.2, input_shape=(781,)))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(10, activation='softmax'))\n",
    "# Multi class -Soft Max and Cross entropy\n",
    "# for Binary Class, sigmoid, Binary crsoo entropy\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "# Optimizer decides how you compute the gradient and how you update the weights\n",
    "# MSE is the metric for regression and accuracy is metric for classification, We want Keras to calculate accuracy\n",
    "# after every epoch\n",
    "\n",
    "nb_epoch = 50      # number of epochs\n",
    "batch_size = 32    # batch size\n",
    "history = mlp.fit(train_data, train_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 94s 9ms/step - loss: 0.6859 - val_loss: 0.3497\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 93s 9ms/step - loss: 0.5702 - val_loss: 0.2860\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 92s 9ms/step - loss: 0.5487 - val_loss: 0.2600\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 98s 10ms/step - loss: 0.5371 - val_loss: 0.2490\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 205s 21ms/step - loss: 0.5331 - val_loss: 0.2373\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 80s 8ms/step - loss: 0.5272 - val_loss: 0.2298\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 82s 8ms/step - loss: 0.5241 - val_loss: 0.2285\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 249s 25ms/step - loss: 0.5237 - val_loss: 0.2246\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 107s 11ms/step - loss: 0.5191 - val_loss: 0.2222\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 105s 10ms/step - loss: 0.5203 - val_loss: 0.2212\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 101s 10ms/step - loss: 0.5155 - val_loss: 0.2167\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 98s 10ms/step - loss: 0.5153 - val_loss: 0.2129\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 1198s 120ms/step - loss: 0.5139 - val_loss: 0.2135\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 80s 8ms/step - loss: 0.5160 - val_loss: 0.2108\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 97s 10ms/step - loss: 0.5082 - val_loss: 0.2116\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 101s 10ms/step - loss: 0.5105 - val_loss: 0.2100\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 90s 9ms/step - loss: 0.5090 - val_loss: 0.2082\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 88s 9ms/step - loss: 0.5106 - val_loss: 0.2088\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 91s 9ms/step - loss: 0.5096 - val_loss: 0.2076\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 91s 9ms/step - loss: 0.5074 - val_loss: 0.2053\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 87s 9ms/step - loss: 0.5078 - val_loss: 0.2042\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 82s 8ms/step - loss: 0.5063 - val_loss: 0.2029\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 251s 25ms/step - loss: 0.5089 - val_loss: 0.2060\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 282s 28ms/step - loss: 0.5056 - val_loss: 0.2034\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 84s 8ms/step - loss: 0.5037 - val_loss: 0.2024\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 83s 8ms/step - loss: 0.5027 - val_loss: 0.2029\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 197s 20ms/step - loss: 0.5034 - val_loss: 0.2029\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 100s 10ms/step - loss: 0.5015 - val_loss: 0.2014\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 94s 9ms/step - loss: 0.5045 - val_loss: 0.2015\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 104s 10ms/step - loss: 0.5011 - val_loss: 0.2003\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 89s 9ms/step - loss: 0.4976 - val_loss: 0.2014\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 95s 9ms/step - loss: 0.4988 - val_loss: 0.1994\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 94s 9ms/step - loss: 0.5011 - val_loss: 0.1990\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 368s 37ms/step - loss: 0.4973 - val_loss: 0.2019\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 107s 11ms/step - loss: 0.4963 - val_loss: 0.2009\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 226s 23ms/step - loss: 0.4980 - val_loss: 0.2009\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 111s 11ms/step - loss: 0.4952 - val_loss: 0.1996\n",
      "Epoch 38/50\n",
      " 3744/10000 [==========>...................] - ETA: 55s - loss: 0.4972"
     ]
    }
   ],
   "source": [
    "# Building an auto-encoder architecture with 'Model' function. This is a bit defferent from 'Sequential' type.\n",
    "# For this, we need to create a series of layers connected together. \n",
    "# Once we have the connections in place, we can use model to define the architecture.\n",
    "# To 'Model', we simply mention the first layer and the last layer.\n",
    "\n",
    "nb_epoch = 50      # number of epochs\n",
    "batch_size = 32    # batch size\n",
    "\n",
    "input_img = Input(shape=(781,))  # input to the Input_img layer original features\n",
    "crrpt_img = Dropout(0.5)(input_img) # input to the crrpt_img is input_img\n",
    "encoded = Dense(1000, activation='sigmoid')(crrpt_img) # INput to the econded layer is Input_img\n",
    "decoded = Dense(781, activation='linear')(encoded) # input to the decoded layer is encoded  \n",
    "\n",
    "autoencoder = Model(input_img,decoded)\n",
    "autoencoder.compile(optimizer='adam',\n",
    "                    loss='mean_squared_error') # This time we are reconstructing so loss functions should be Mean squred error\n",
    "\n",
    "history = autoencoder.fit(train_data, train_data,  \n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(test_data, test_data))\n",
    "\n",
    "autoencoder.save('DAE_l1_model.h5') # save the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For predicting encoded output, we can define a model which starts with input layer and ends with encoded layer\n",
    "# Since these layers are already trained, we can directly predict the encoded values\n",
    "\n",
    "encoder = Model(input_img,encoded)\n",
    "htrain_data = encoder.predict(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP for classifying cifar data using encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train autoencoder to get the feature extractions\n",
    "from keras.optimizers import Adam\n",
    "adam = Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10000, 1000), (10000, 10))\n",
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s - loss: 2.0458 - acc: 0.2645 - val_loss: 1.8417 - val_acc: 0.3386\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.8226 - acc: 0.3504 - val_loss: 1.7803 - val_acc: 0.3662\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.7520 - acc: 0.3779 - val_loss: 1.7306 - val_acc: 0.3822\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.7034 - acc: 0.3979 - val_loss: 1.7332 - val_acc: 0.3840\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.6570 - acc: 0.4159 - val_loss: 1.7385 - val_acc: 0.3847\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.6137 - acc: 0.4310 - val_loss: 1.6896 - val_acc: 0.3938\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.5799 - acc: 0.4465 - val_loss: 1.6942 - val_acc: 0.3964\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.5386 - acc: 0.4622 - val_loss: 1.6447 - val_acc: 0.4174\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.5048 - acc: 0.4618 - val_loss: 1.6501 - val_acc: 0.4198\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.4584 - acc: 0.4867 - val_loss: 1.6402 - val_acc: 0.4176\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.4250 - acc: 0.4964 - val_loss: 1.6087 - val_acc: 0.4316\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.3852 - acc: 0.5178 - val_loss: 1.5805 - val_acc: 0.4402\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.3638 - acc: 0.5159 - val_loss: 1.5731 - val_acc: 0.4440\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.3337 - acc: 0.5321 - val_loss: 1.5744 - val_acc: 0.4428\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2842 - acc: 0.5460 - val_loss: 1.5532 - val_acc: 0.4496\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2638 - acc: 0.5548 - val_loss: 1.5700 - val_acc: 0.4390\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2241 - acc: 0.5727 - val_loss: 1.5412 - val_acc: 0.4536\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1910 - acc: 0.5780 - val_loss: 1.5579 - val_acc: 0.4559\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1617 - acc: 0.5912 - val_loss: 1.5380 - val_acc: 0.4582\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1252 - acc: 0.6086 - val_loss: 1.5615 - val_acc: 0.4450\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0839 - acc: 0.6261 - val_loss: 1.5274 - val_acc: 0.4622\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0612 - acc: 0.6300 - val_loss: 1.5382 - val_acc: 0.4605\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0118 - acc: 0.6490 - val_loss: 1.5605 - val_acc: 0.4566\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9968 - acc: 0.6520 - val_loss: 1.5487 - val_acc: 0.4603\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9607 - acc: 0.6638 - val_loss: 1.5449 - val_acc: 0.4588\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9352 - acc: 0.6753 - val_loss: 1.5458 - val_acc: 0.4653\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.8904 - acc: 0.6944 - val_loss: 1.5516 - val_acc: 0.4673\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.8723 - acc: 0.6963 - val_loss: 1.5421 - val_acc: 0.4722\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.8553 - acc: 0.7060 - val_loss: 1.5706 - val_acc: 0.4606\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.8215 - acc: 0.7183 - val_loss: 1.5472 - val_acc: 0.4642\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.7995 - acc: 0.7258 - val_loss: 1.5365 - val_acc: 0.4707\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.7662 - acc: 0.7403 - val_loss: 1.5651 - val_acc: 0.4662\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.7438 - acc: 0.7387 - val_loss: 1.5625 - val_acc: 0.4690\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.7140 - acc: 0.7551 - val_loss: 1.5673 - val_acc: 0.4669\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6890 - acc: 0.7696 - val_loss: 1.5661 - val_acc: 0.4694\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6730 - acc: 0.7749 - val_loss: 1.5720 - val_acc: 0.4694\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6458 - acc: 0.7808 - val_loss: 1.5906 - val_acc: 0.4679\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6245 - acc: 0.7882 - val_loss: 1.5689 - val_acc: 0.4714\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5999 - acc: 0.7960 - val_loss: 1.6138 - val_acc: 0.4701\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5965 - acc: 0.7965 - val_loss: 1.5817 - val_acc: 0.4743\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5653 - acc: 0.8093 - val_loss: 1.5862 - val_acc: 0.4731\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5465 - acc: 0.8180 - val_loss: 1.5876 - val_acc: 0.4708\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5513 - acc: 0.8123 - val_loss: 1.6136 - val_acc: 0.4667\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5169 - acc: 0.8254 - val_loss: 1.5900 - val_acc: 0.4754\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5089 - acc: 0.8302 - val_loss: 1.6528 - val_acc: 0.4679\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4888 - acc: 0.8346 - val_loss: 1.6172 - val_acc: 0.4732\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4744 - acc: 0.8406 - val_loss: 1.6775 - val_acc: 0.4684\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4762 - acc: 0.8368 - val_loss: 1.6430 - val_acc: 0.4710\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4518 - acc: 0.8449 - val_loss: 1.6464 - val_acc: 0.4748\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4394 - acc: 0.8519 - val_loss: 1.6689 - val_acc: 0.4716\n"
     ]
    }
   ],
   "source": [
    "# training an MLP on autoencoder features\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dropout(0.2, input_shape=(1000,)))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(10, activation='softmax'))\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "htest_data = encoder.predict(test_data)\n",
    "print(htest_data.shape, test_labels.shape)\n",
    "history = mlp.fit(htrain_data[:10000], train_labels[:10000],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    validation_data=(htest_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
