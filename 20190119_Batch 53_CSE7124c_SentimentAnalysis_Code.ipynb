{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis, the ML way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given to you, is a collection of short reviews of some movies. The reviews sound positive or negative about the movie. Though we, as humans, can easily identify the sentiment of the text by looking/reading the words in a sentence, it is very difficult to teach a machine/system to understand the sentiment in a given text.\n",
    "\n",
    "One way is the ML way. There is a ground truth that is created for some corpus i.e.,  we have both postive and negative reviews that are tagged with their respective classes. This forms the base and the algorithm is trained on this data (after converting this to structured form) and depending on the words used, the classification is done (Machine/system tries to obtain a pattern from data).\n",
    "\n",
    "Another way is dictionary approach, where we create a dictionary of positive and negative words and explicitly state that these words are positive or negative. We can then count the number of positive and negative words in the sentence and give a score. If the score is positive then its positive else its negative.\n",
    "\n",
    "In either cases, there is some manual work involved (creating ground truth in case 1 or creating the dictionary in case 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maheshkumar/DataBox/INSOFE/Academics/PGP/Batch 45/CSE 7124c/20180916_Batch_45_CSE7124c_SentimentAnalysis_LSA_Day02_Lab_Content/20180516_Batch45_CSE7124c_SentimentAnalysis_Code'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maheshkumar/DataBox/INSOFE/Academics/PGP/Batch 45/CSE 7124c/20180916_Batch_45_CSE7124c_SentimentAnalysis_LSA_Day02_Lab_Content/20180916_Bacth45_CSE7124c_TextMining_Lab02_Datasets'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/maheshkumar/DataBox/INSOFE/Academics/PGP/Batch 45/CSE 7124c/20180916_Batch_45_CSE7124c_SentimentAnalysis_LSA_Day02_Lab_Content/20180916_Bacth45_CSE7124c_TextMining_Lab02_Datasets'\n",
    "os.chdir(path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(\"short_reviews/positive.txt\",\"r\", encoding=\"latin\")   # \"r\" is for reading\n",
    "short_pos = f1.readlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth . \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_pos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(short_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_pos=[re.sub(\"\\n\",\"\",i)for i in short_pos]\n",
    "x_short_pos=short_pos[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2 = open(\"short_reviews/negative.txt\",\"r\",encoding=\"latin\")\n",
    "short_neg = f2.readlines()\n",
    "short_neg=[re.sub(\"\\n\",\"\",i)for i in short_neg]\n",
    "x_short_neg=short_neg[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(short_neg[:1000])\n",
    "#print#(hort_pos[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine the first 1000 positive reviews and first 1000 negative reviews to form a corpus\n",
    "data = x_short_pos + x_short_neg\n",
    "\n",
    "#create the target variable representing 1000 'pos' and 'neg' instances each, wrt the data created above\n",
    "target = ['pos']*1000+['neg']*1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 6435)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get doc-term matrix on train data\n",
    "cv=CountVectorizer(stop_words='english',lowercase=True,\n",
    "                   strip_accents='unicode',decode_error='ignore')\n",
    "\n",
    "tdm_train = cv.fit_transform(X_train)\n",
    "Mat = tdm_train.todense()\n",
    "Mat\n",
    "Mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 6435)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get doc-term matrix for test data\n",
    "tdm_test = cv.transform(X_test)\n",
    "Mat_test = tdm_test.todense()\n",
    "Mat_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #just to ignore any warnings\n",
    "\n",
    "#Training the model\n",
    "logreg = LogisticRegression()\n",
    "lr_clf = logreg.fit(tdm_train, y_train)\n",
    "\n",
    "\n",
    "#Predicting on train data\n",
    "train_pred = lr_clf.predict(tdm_train)\n",
    "\n",
    "\n",
    "#Predicting on test data\n",
    "test_pred=lr_clf.predict(tdm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Confusion Matrix: \n",
      " [[794   4]\n",
      " [  5 797]]\n",
      "Test_Confusion Matrix: \n",
      " [[145  57]\n",
      " [ 62 136]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train_Confusion Matrix: \\n\", confusion_matrix(y_train,train_pred))\n",
    "print(\"Test_Confusion Matrix: \\n\", confusion_matrix(y_test,test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A glance of first 10 values:\n",
      " \n",
      " Test Actuals:  ['neg', 'pos', 'neg', 'pos', 'pos', 'neg', 'neg', 'neg', 'neg', 'pos'] \n",
      " Test Predictions:  ['neg' 'neg' 'neg' 'pos' 'pos' 'neg' 'pos' 'neg' 'neg' 'pos']\n"
     ]
    }
   ],
   "source": [
    "print(\"A glance of first 10 values:\\n\", \"\\n Test Actuals: \", y_test[:10],\"\\n\", \"Test Predictions: \", test_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of logistic regression on train data: \n",
      "Accuracy: 0.994375 \n",
      "Recall: 0.994987468672 \n",
      "Precision: 0.993742177722\n"
     ]
    }
   ],
   "source": [
    "#Train Metrics\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_train, train_pred)\n",
    "rec = recall_score(y_train, train_pred, pos_label='neg')\n",
    "prec = precision_score(y_train, train_pred, pos_label='neg')\n",
    "\n",
    "print(\"Results of logistic regression on train data:\",\"\\nAccuracy:\",acc, \"\\nRecall:\",rec, \"\\nPrecision:\",prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of logistic regression on test data: \n",
      "Accuracy: 0.7025 \n",
      "Recall: 0.717821782178 \n",
      "Precision: 0.700483091787\n"
     ]
    }
   ],
   "source": [
    "#Test Metrics\n",
    "acc = accuracy_score(y_test, test_pred)\n",
    "rec = recall_score(y_test, test_pred, pos_label='neg')\n",
    "prec = precision_score(y_test, test_pred, pos_label='neg')\n",
    "\n",
    "print(\"Results of logistic regression on test data:\",\"\\nAccuracy:\",acc, \"\\nRecall:\",rec, \"\\nPrecision:\",prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work with any other classification model and check if you can improve the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[137,  65],\n",
       "       [ 55, 143]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(tdm_train, y_train)\n",
    "test_pred = classifier.predict(tdm_test)\n",
    "confusion_matrix(y_test,test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Multinomial_NB on test data: \n",
      "Accuracy: 0.7 \n",
      "Recall: 0.678217821782 \n",
      "Precision: 0.713541666667\n"
     ]
    }
   ],
   "source": [
    "#Test Metrics\n",
    "acc = accuracy_score(y_test, test_pred)\n",
    "rec = recall_score(y_test, test_pred, pos_label='neg')\n",
    "prec = precision_score(y_test, test_pred, pos_label='neg')\n",
    "\n",
    "print(\"Results of Multinomial_NB on test data:\",\"\\nAccuracy:\",acc, \"\\nRecall:\",rec, \"\\nPrecision:\",prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else could be done to improve the accuracies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: There could be some common words in both positive and negative reviews.\n",
    "    To avoid such words we can consider only adjectives to solve the problem and check if the accuracies improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
